% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{ctex}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={法国机动车第三者责任保险案例分析},
  pdfauthor={张连增},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{法国机动车第三者责任保险案例分析}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{前馈神经网络（FNN）}
\author{张连增}
\date{2024-07-09}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\subsection{介绍}\label{ux4ecbux7ecd}

在本文档中，我们将探讨使用前馈神经网络（FNN）来分析法国机动车第三者责任索赔。我们将讨论架构选择、缺失值处理以及各种练习，以了解模型的性能和依赖性。

此 notebook 是 SSRN
上的``\href{https://www.ssrn.com/}{来自神经网络内部的见解}''教程的配套教材。

该代码与上面教程中使用的代码相似，并结合了脚本中的原始 R 代码，可在
GitHub 上获得包含更多注解的内容。请参考教程来获取解释。

请注意，结果可能会因 R 和 Python 包版本而异，有关 \texttt{sessionInfo()}
的结果和 Python 设置的相应信息，请参见最后一节。

\subsection{数据准备}\label{ux6570ux636eux51c6ux5907}

本教程使用 openML \href{https://www.openml.org/d/41214}{openML (ID
41214)} 上可用的 French MTPL 数据集。

\subsubsection{加载包和数据}\label{ux52a0ux8f7dux5305ux548cux6570ux636e}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}
\FunctionTok{library}\NormalTok{(magrittr)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(tibble)}
\FunctionTok{library}\NormalTok{(purrr)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(splitTools)}
\FunctionTok{library}\NormalTok{(tidyr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plotting parameters in R Markdown notebook}
\NormalTok{knitr}\SpecialCharTok{::}\NormalTok{opts\_chunk}\SpecialCharTok{$}\FunctionTok{set}\NormalTok{(}\AttributeTok{fig.width =} \DecValTok{9}\NormalTok{, }\AttributeTok{fig.height =} \DecValTok{9}\NormalTok{)}
\CommentTok{\# plotting parameters in Jupyter notebook}
\FunctionTok{library}\NormalTok{(repr)  }\CommentTok{\# only needed for Jupyter notebook}
\FunctionTok{options}\NormalTok{(}\AttributeTok{repr.plot.width =} \DecValTok{9}\NormalTok{, }\AttributeTok{repr.plot.height =} \DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{设置全局参数}\label{ux8bbeux7f6eux5168ux5c40ux53c2ux6570}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{encoding =} \StringTok{\textquotesingle{}UTF{-}8\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set seed to obtain best reproducibility. note that the underlying architecture may affect results nonetheless, so full reproducibility cannot be guaranteed across different platforms.}
\NormalTok{seed }\OtherTok{\textless{}{-}} \DecValTok{100}
\FunctionTok{Sys.setenv}\NormalTok{(}\AttributeTok{PYTHONHASHSEED =}\NormalTok{ seed)}
\FunctionTok{set.seed}\NormalTok{(seed)}
\NormalTok{reticulate}\SpecialCharTok{::}\FunctionTok{py\_set\_seed}\NormalTok{(seed)}
\NormalTok{tensorflow}\SpecialCharTok{::}\NormalTok{tf}\SpecialCharTok{$}\NormalTok{random}\SpecialCharTok{$}\FunctionTok{set\_seed}\NormalTok{(seed)}
\end{Highlighting}
\end{Shaded}

下面的结果与论文中的结果不完全匹配，因为底层数据集和一些包是不同的。此外，训练和测试数据的划分也不同。但是，总的结论是相同的。

\subsubsection{辅助函数}\label{ux8f85ux52a9ux51fdux6570}

随后，为了便于阅读，我们在本节中提供了本教程中使用的所有辅助函数。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summarize }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(...) }\FunctionTok{suppressMessages}\NormalTok{(dplyr}\SpecialCharTok{::}\FunctionTok{summarize}\NormalTok{(...))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{load\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(file) \{}
  \FunctionTok{load}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\StringTok{"./0\_data"}\NormalTok{, file), }\AttributeTok{envir =} \FunctionTok{parent.frame}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Poisson deviance}
\NormalTok{PoissonDeviance }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pred, obs) \{}
    \DecValTok{200} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(pred) }\SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(obs) }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{((obs}\SpecialCharTok{/}\NormalTok{pred)}\SpecialCharTok{\^{}}\NormalTok{(obs)))) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(pred)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_freq }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(test, xvar, title, model, mdlvariant) \{}
\NormalTok{  out }\OtherTok{\textless{}{-}}\NormalTok{ test }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(}\SpecialCharTok{!!}\FunctionTok{sym}\NormalTok{(xvar)) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{summarize}\NormalTok{(}\AttributeTok{obs =} \FunctionTok{sum}\NormalTok{(ClaimNb) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Exposure), }\AttributeTok{pred =} \FunctionTok{sum}\NormalTok{(}\SpecialCharTok{!!}\FunctionTok{sym}\NormalTok{(mdlvariant)) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Exposure))}
  
  \FunctionTok{ggplot}\NormalTok{(out, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \SpecialCharTok{!!}\FunctionTok{sym}\NormalTok{(xvar), }\AttributeTok{group =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ pred, }\AttributeTok{colour =}\NormalTok{ model)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ obs, }\AttributeTok{colour =} \StringTok{"observed"}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ pred, }\AttributeTok{colour =}\NormalTok{ model), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ obs, }\AttributeTok{colour =} \StringTok{"observed"}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.35}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =}\NormalTok{ xvar, }\AttributeTok{y =} \StringTok{"frequency"}\NormalTok{, }\AttributeTok{title =}\NormalTok{ title) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_loss }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(x) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) \{}
\NormalTok{    df\_val }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{epoch =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(x}\SpecialCharTok{$}\NormalTok{loss), }\AttributeTok{train\_loss =}\NormalTok{ x}\SpecialCharTok{$}\NormalTok{loss, }\AttributeTok{val\_loss =}\NormalTok{ x}\SpecialCharTok{$}\NormalTok{val\_loss)}
\NormalTok{    df\_val }\OtherTok{\textless{}{-}} \FunctionTok{gather}\NormalTok{(df\_val, variable, loss, }\SpecialCharTok{{-}}\NormalTok{epoch)}
\NormalTok{    p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df\_val, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ epoch, }\AttributeTok{y =}\NormalTok{ loss)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{variable, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{geom\_smooth}\NormalTok{()}
    \FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{print}\NormalTok{(p))}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsubsection{加载数据}\label{ux52a0ux8f7dux6570ux636e}

我们考虑包含在 R 包 CASdatasets 中的数据集 \texttt{freMTPL2freq}
用于索赔频率建模。该数据包括法国汽车第三方责任 (MTPL)
保险组合，以及在一个会计年度观察到的相应索赔次数。我们没有纳入索赔规模，这也可以通过
\texttt{freMTPL2sev} 获取。

由于当前的包版本提供了一个稍微修改过的数据集，我们使用 openML (ID 41214)
上可用的旧数据集。在我们可以使用这个数据集之前，我们需要做一些数据清理。F.
Loser 指出，某些索赔次数似乎不正确。因此，我们使用 \emph{Statistical
Foundations of Actuarial Learning and its Applications} 一书中附录 A.1
中描述的数据预处理方法。可以从此处的课程
\href{https://github.com/actuarial-data-science/CourseDeepLearningWithActuarialApplications}{GitHub}
页面下载此预处理数据。

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load\_data}\NormalTok{(}\StringTok{"freMTPL2freq.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{数据预处理}\label{ux6570ux636eux9884ux5904ux7406}

先验地，没有足够的关于这些数据的信息来对风险暴露数测量的最佳考虑做出明智的决定，无论是作为特征还是作为偏移量。在下文中，我们始终将风险暴露数视为偏移量。

数据预处理包括几个转换。我们确保 \texttt{ClaimNb}
是一个整数，\texttt{VehAge}、\texttt{DrivAge} 和 \texttt{BonusMalus}
分别在 20 岁、90 岁和 150
的水平上设置了上限，以提高可视化效果。\texttt{Density}
是对数的，\texttt{VehGas} 是一个分类变量。我们放弃了第一个 notebook
中使用的四舍五入，它主要用于更好地可视化数据。

我们正在添加一个 \texttt{group\_id}
标识可能引用相同保单的行。在数据拆分技术（训练/测试、交叉验证）中考虑
\texttt{group\_id}
是必不可少的。这与使用了另一种拆分方法的教程不同。因此，本 notebook
中的图形与教程中的图形不匹配，但得出的结论是相同的。

我们决定截断索赔次数和索赔金额以纠正不合理的数据输入并简化建模部分。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Grouping id}
\NormalTok{distinct }\OtherTok{\textless{}{-}}\NormalTok{ freMTPL2freq }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(IDpol, Exposure, ClaimNb))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group\_id =} \FunctionTok{row\_number}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ freMTPL2freq }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{left\_join}\NormalTok{(distinct) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ClaimNb =} \FunctionTok{pmin}\NormalTok{(}\FunctionTok{as.integer}\NormalTok{(ClaimNb), }\DecValTok{4}\NormalTok{),}
         \AttributeTok{VehAge =} \FunctionTok{pmin}\NormalTok{(VehAge, }\DecValTok{20}\NormalTok{),}
         \AttributeTok{DrivAge =} \FunctionTok{pmin}\NormalTok{(DrivAge, }\DecValTok{90}\NormalTok{),}
         \AttributeTok{BonusMalus =} \FunctionTok{pmin}\NormalTok{(BonusMalus, }\DecValTok{150}\NormalTok{),}
         \AttributeTok{Density =} \FunctionTok{round}\NormalTok{(}\FunctionTok{log}\NormalTok{(Density), }\DecValTok{2}\NormalTok{),}
         \AttributeTok{VehGas =} \FunctionTok{factor}\NormalTok{(VehGas),}
         \AttributeTok{Exposure =} \FunctionTok{pmin}\NormalTok{(Exposure, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Group sizes of suspected clusters}
\FunctionTok{table}\NormalTok{(}\FunctionTok{table}\NormalTok{(dat[, }\StringTok{"group\_id"}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##      1      2      3      4      5      6      7      8      9     10     11 
## 429576  84201  13940   2437    966    754    720    475    400    269    142 
##     12     13     14     15     18     22 
##    191      3      1      2      1      1
\end{verbatim}

\subsubsection{关于广义线性模型的特征预处理}\label{ux5173ux4e8eux5e7fux4e49ux7ebfux6027ux6a21ux578bux7684ux7279ux5f81ux9884ux5904ux7406}

如前所述，通常特征\(x_i\)在用于特定模型之前需要进行预处理。在我们的
Poisson GLM
中，回归函数由连续特征分量中的对数-线性形式建模。在这里做出以下选择：

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{Area}}：我们为\(\{A,\ldots,F\}\mapsto\{1,\ldots,6\}\)选择一个连续（对数-线性）特征分量；
\item
  \textbf{\texttt{VehPower}}：我们选择一个分类特征，其中我们合并大于等于
  9 的车辆功率组（总共 6 个类别）；
\item
  \textbf{\texttt{VehAge}}：我们建立 3
  个分类类别\([0,1),[1,10],(10,\infty);\leftrightarrow\)；
\item
  \textbf{\texttt{DrivAge}}：我们构建了 7
  个分类类别\([18,21),[21,26),[26,31),[31,41),[41,51),[51,71),[71,\infty);~\leftrightarrow\)；
\item
  \textbf{\texttt{BonusMalus}}：连续对数-线性特征（上限设为 150）；
\item
  \textbf{\texttt{VehBrand}}：分类特征（共11类）；
\item
  \textbf{\texttt{VehGas}}：二分类特征；
\item
  \textbf{\texttt{Density}}：log-density
  被选为连续的对数-线性特征组件（请注意，对于小的
  log-densities，我们的数值非常小）；
\item
  \textbf{\texttt{Region}}：分类特征组件（共22类）；
\end{itemize}

因此，我们考虑 3
个连续特征分量（\texttt{Area}、\texttt{BonusMalus}、\texttt{log-Density}）、1
个二元特征分量（\texttt{VehGas}）和 5
个分类特征分量（\texttt{VehPower}、\texttt{VehAge}、\texttt{DrivAge}、\texttt{VehBrand}、\texttt{Region}）。\texttt{VehPower}、\texttt{VehAge}
和 \texttt{DrivAge}
的这些分类类别仅根据专家意见完成。该专家意见试图在类别标签中找到同质性，并且每个类别标签都应获得足够的（观察）量。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat2 }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}
  \AttributeTok{AreaGLM =} \FunctionTok{as.integer}\NormalTok{(Area),}
  \AttributeTok{VehPowerGLM =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(VehPower, }\DecValTok{9}\NormalTok{)),}
  \AttributeTok{VehAgeGLM =} \FunctionTok{cut}\NormalTok{(VehAge, }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\ConstantTok{Inf}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{,}\StringTok{"2"}\NormalTok{,}\StringTok{"3"}\NormalTok{)),}
  \AttributeTok{DrivAgeGLM =} \FunctionTok{cut}\NormalTok{(DrivAge, }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{70}\NormalTok{, }\ConstantTok{Inf}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{,}\StringTok{"2"}\NormalTok{,}\StringTok{"3"}\NormalTok{,}\StringTok{"4"}\NormalTok{,}\StringTok{"5"}\NormalTok{,}\StringTok{"6"}\NormalTok{,}\StringTok{"7"}\NormalTok{)),}
  \AttributeTok{BonusMalusGLM =} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(BonusMalus, }\DecValTok{150}\NormalTok{)),}
  \AttributeTok{DensityGLM =} \FunctionTok{as.numeric}\NormalTok{(Density),}
  \AttributeTok{VehAgeGLM =} \FunctionTok{relevel}\NormalTok{(VehAgeGLM, }\AttributeTok{ref =} \StringTok{"2"}\NormalTok{),   }
  \AttributeTok{DrivAgeGLM =} \FunctionTok{relevel}\NormalTok{(DrivAgeGLM, }\AttributeTok{ref =} \StringTok{"5"}\NormalTok{),}
  \AttributeTok{Region =} \FunctionTok{relevel}\NormalTok{(Region, }\AttributeTok{ref =} \StringTok{"R24"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

我们注意到，对于分类变量，我们使用 R
中的数据类型因子。这种数据类型在相应的 R
过程中自动考虑虚拟编码。分类变量被初始化为一个类（参考水平）。我们通常初始化为具有最大容量的类。这种初始化是通过命令
relevel
实现的，见上文。此初始化不会影响拟合均值，但会提供独特的参数化。有关详细信息，请参阅
\texttt{?relevel}。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?relevel}
\end{Highlighting}
\end{Shaded}

\subsubsection{检查准备好的数据集}\label{ux68c0ux67e5ux51c6ux5907ux597dux7684ux6570ux636eux96c6}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(dat2))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0331}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0497}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0276}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0497}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0387}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0442}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0608}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0497}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0442}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0442}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0387}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0608}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0442}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0497}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0442}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0663}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0552}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0608}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0773}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0608}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
IDpol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Exposure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Area
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
VehPower
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
VehAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DrivAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BonusMalus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VehBrand
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VehGas
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Density
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ClaimTotal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ClaimNb
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
group\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AreaGLM
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VehPowerGLM
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VehAgeGLM
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
DrivAgeGLM
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BonusMalusGLM
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DensityGLM
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0.10 & D & 5 & 0 & 55 & 50 & B12 & Regular & 7.10 & R82 & 0 & 0 & 1
& 4 & 5 & 1 & 6 & 50 & 7.10 \\
3 & 0.77 & D & 5 & 0 & 55 & 50 & B12 & Regular & 7.10 & R82 & 0 & 0 & 1
& 4 & 5 & 1 & 6 & 50 & 7.10 \\
5 & 0.75 & B & 6 & 2 & 52 & 50 & B12 & Diesel & 3.99 & R22 & 0 & 0 & 2 &
2 & 6 & 2 & 6 & 50 & 3.99 \\
10 & 0.09 & B & 7 & 0 & 46 & 50 & B12 & Diesel & 4.33 & R72 & 0 & 0 & 3
& 2 & 7 & 1 & 5 & 50 & 4.33 \\
11 & 0.84 & B & 7 & 0 & 46 & 50 & B12 & Diesel & 4.33 & R72 & 0 & 0 & 3
& 2 & 7 & 1 & 5 & 50 & 4.33 \\
13 & 0.52 & E & 6 & 2 & 38 & 50 & B12 & Regular & 8.01 & R31 & 0 & 0 & 4
& 5 & 6 & 2 & 4 & 50 & 8.01 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(dat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    678007 obs. of  20 variables:
##  $ IDpol        : num  1 3 5 10 11 13 15 17 18 21 ...
##  $ Exposure     : num  0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ...
##  $ Area         : Factor w/ 6 levels "A","B","C","D",..: 4 4 2 2 2 5 5 3 3 2 ...
##  $ VehPower     : num  5 5 6 7 7 6 6 7 7 7 ...
##  $ VehAge       : num  0 0 2 0 0 2 2 0 0 0 ...
##  $ DrivAge      : num  55 55 52 46 46 38 38 33 33 41 ...
##  $ BonusMalus   : num  50 50 50 50 50 50 50 68 68 50 ...
##  $ VehBrand     : Factor w/ 11 levels "B1","B2","B3",..: 9 9 9 9 9 9 9 9 9 9 ...
##  $ VehGas       : Factor w/ 2 levels "Diesel","Regular": 2 2 1 1 1 2 2 1 1 1 ...
##  $ Density      : num  7.1 7.1 3.99 4.33 4.33 8.01 8.01 4.92 4.92 4.09 ...
##  $ Region       : Factor w/ 22 levels "R24","R11","R21",..: 18 18 4 15 15 8 8 20 20 12 ...
##  $ ClaimTotal   : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ ClaimNb      : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ group_id     : int  1 1 2 3 3 4 4 5 5 6 ...
##  $ AreaGLM      : int  4 4 2 2 2 5 5 3 3 2 ...
##  $ VehPowerGLM  : Factor w/ 6 levels "4","5","6","7",..: 2 2 3 4 4 3 3 4 4 4 ...
##  $ VehAgeGLM    : Factor w/ 3 levels "2","1","3": 2 2 1 2 2 1 1 2 2 2 ...
##  $ DrivAgeGLM   : Factor w/ 7 levels "5","1","2","3",..: 6 6 6 1 1 5 5 5 5 1 ...
##  $ BonusMalusGLM: int  50 50 50 50 50 50 50 68 68 50 ...
##  $ DensityGLM   : num  7.1 7.1 3.99 4.33 4.33 8.01 8.01 4.92 4.92 4.09 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(dat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      IDpol            Exposure        Area          VehPower     
##  Min.   :      1   Min.   :0.002732   A:103957   Min.   : 4.000  
##  1st Qu.:1157948   1st Qu.:0.180000   B: 75459   1st Qu.: 5.000  
##  Median :2272153   Median :0.490000   C:191880   Median : 6.000  
##  Mean   :2621857   Mean   :0.528547   D:151590   Mean   : 6.455  
##  3rd Qu.:4046278   3rd Qu.:0.990000   E:137167   3rd Qu.: 7.000  
##  Max.   :6114330   Max.   :1.000000   F: 17954   Max.   :15.000  
##                                                                  
##      VehAge          DrivAge       BonusMalus        VehBrand     
##  Min.   : 0.000   Min.   :18.0   Min.   : 50.00   B12    :166024  
##  1st Qu.: 2.000   1st Qu.:34.0   1st Qu.: 50.00   B1     :162730  
##  Median : 6.000   Median :44.0   Median : 50.00   B2     :159861  
##  Mean   : 6.976   Mean   :45.5   Mean   : 59.76   B3     : 53395  
##  3rd Qu.:11.000   3rd Qu.:55.0   3rd Qu.: 64.00   B5     : 34753  
##  Max.   :20.000   Max.   :90.0   Max.   :150.00   B6     : 28548  
##                                                   (Other): 72696  
##      VehGas          Density           Region         ClaimTotal     
##  Diesel :332136   Min.   : 0.000   R24    :160601   Min.   :      0  
##  Regular:345871   1st Qu.: 4.520   R82    : 84752   1st Qu.:      0  
##                   Median : 5.970   R93    : 79315   Median :      0  
##                   Mean   : 5.982   R11    : 69791   Mean   :     88  
##                   3rd Qu.: 7.410   R53    : 42122   3rd Qu.:      0  
##                   Max.   :10.200   R52    : 38751   Max.   :4075401  
##                                    (Other):202675                    
##     ClaimNb           group_id         AreaGLM     VehPowerGLM VehAgeGLM 
##  Min.   :0.00000   Min.   :     1   Min.   :1.00   4:115343    2:434492  
##  1st Qu.:0.00000   1st Qu.:149319   1st Qu.:2.00   5:124821    1: 57739  
##  Median :0.00000   Median :273211   Median :3.00   6:148976    3:185776  
##  Mean   :0.03891   Mean   :275320   Mean   :3.29   7:145401              
##  3rd Qu.:0.00000   3rd Qu.:404072   3rd Qu.:4.00   8: 46956              
##  Max.   :4.00000   Max.   :534079   Max.   :6.00   9: 96510              
##                                                                          
##  DrivAgeGLM BonusMalusGLM      DensityGLM    
##  5:165185   Min.   : 50.00   Min.   : 0.000  
##  1:  6816   1st Qu.: 50.00   1st Qu.: 4.520  
##  2: 32079   Median : 50.00   Median : 5.970  
##  3: 65594   Mean   : 59.76   Mean   : 5.982  
##  4:170097   3rd Qu.: 64.00   3rd Qu.: 7.410  
##  6:198871   Max.   :150.00   Max.   :10.200  
##  7: 39365
\end{verbatim}

\subsubsection{划分训练和测试数据集}\label{ux5212ux5206ux8badux7ec3ux548cux6d4bux8bd5ux6570ux636eux96c6}

首先，我们将数据集拆分为训练和测试。由于保单中潜在的行分组，我们不能只进行随机拆分。为此，我们使用
\texttt{splitTools} 包中的函数 \texttt{partition(...)}。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ind }\OtherTok{\textless{}{-}} \FunctionTok{partition}\NormalTok{(dat2[[}\StringTok{"group\_id"}\NormalTok{]], }\AttributeTok{p =} \FunctionTok{c}\NormalTok{(}\AttributeTok{train =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{test =} \FloatTok{0.2}\NormalTok{), }
                 \AttributeTok{seed =}\NormalTok{ seed, }\AttributeTok{type =} \StringTok{"grouped"}\NormalTok{)}
\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ dat2[ind}\SpecialCharTok{$}\NormalTok{train, ]}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ dat2[ind}\SpecialCharTok{$}\NormalTok{test, ]}
\end{Highlighting}
\end{Shaded}

它描述了我们对学习数据集\(\mathcal{D}\)和测试数据集\(\mathcal{T}\)的选择，即我们将
80\% 的保单随机分配给\(\mathcal{D}\),将其余 20\%
的保单分配给\(\mathcal{T}\)。4 通常，90/10 或
80/20用于划分训练和测试数据。这是建模的经验法则和最佳实践。可以在\href{https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio}{这里}找到一个很好的解释，引用如下：``有两个相互竞争的问题：训练数据越少，参数估计的方差越大；测试数据越少，性能统计数据的方差就越大。一般来说，你应该关注划分数据使得两组方差都不会太高，这更多地与每个类别中的实例的绝对数量有关，而不是百分比。''

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# size of train/test}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of observations (train): \%s"}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(train))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of observations (train): 542331"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of observations (test): \%s"}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of observations (test): 135676"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Claims frequency of train/test}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Empirical frequency (train): \%s"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{ClaimNb) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Empirical frequency (train): 0.0736"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Empirical frequency (test): \%s"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{ClaimNb) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Empirical frequency (test): 0.0736"
\end{verbatim}

\subsection{存储模型结果}\label{ux5b58ux50a8ux6a21ux578bux7ed3ux679c}

当我们要比较各种模型时，我们创建了一个表，其中存储了我们将用于比较和选择最佳模型的指标。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# initialize table to store all model results for comparison}
\NormalTok{df\_cmp }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
 \AttributeTok{model =} \FunctionTok{character}\NormalTok{(),}
 \AttributeTok{epochs =} \FunctionTok{numeric}\NormalTok{(),}
 \AttributeTok{run\_time =} \FunctionTok{numeric}\NormalTok{(),}
 \AttributeTok{parameters =} \FunctionTok{numeric}\NormalTok{(),}
 \AttributeTok{in\_sample\_loss =} \FunctionTok{numeric}\NormalTok{(),}
 \AttributeTok{out\_sample\_loss =} \FunctionTok{numeric}\NormalTok{(),}
 \AttributeTok{avg\_freq =} \FunctionTok{numeric}\NormalTok{(),}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

在接下来的章节中，我们将为数据拟合各种前馈神经网络。最后，我们将比较几个拟合模型的性能和好坏。我们使用上面定义的指标来比较它们。

\subsubsection{模型假设}\label{ux6a21ux578bux5047ux8bbe}

在拟合任何神经网络之前，我们先拟合一个作为基准模型的广义线性模型（GLM）。
在下文中，我们将基于泊松假设拟合各种索赔频率模型，更准确地说，我们做出以下假设：

选择特征空间\(\chi\)并定义回归函数
\(\lambda:\chi\to\mathbb{R}_+\)如下：\(\leftrightarrow\)
\[\mathbf{x}\mapsto\log\lambda(\mathbf{x})=\beta_0+\sum_{l=1}^{q_0}\beta_lx_l\stackrel{\mathrm{def.}}{=}<\beta,\mathbf{x}>\:,\]

其中，参数向量\(\beta=(\beta_0,\ldots,\beta_{q_0})^{\prime}\in\mathbb{R}^{q_0+1}\)。我们假设对于\(i\geq1,\)
\[N_i\stackrel{\mathrm{ind.}}{\sim}\mathrm{Poi}(\lambda(x_i)v_i)\]
要解决的主要问题是找到回归函数\(\lambda\)(-),使其能够恰当地描述数据，并泛化到尚未见过的相似数据。注意寻找合适的回归函数的任务：\(\lambda:\chi\to\mathbb{R}_+\)还包括特征空间\(\chi\)的定义，它通常随不同的建模方法而变化。

\subsection{模型1：GLM}\label{ux6a21ux578b1glm}

\subsubsection{定义}\label{ux5b9aux4e49}

定义的特征组件本质上是连续的，但我们为了建模目的一直将它们转换为分类的(如上所述)。有了这么多数据，我们可以进一步探索这些分类特征组件，方法是假设适当的连续函数形式，尝试用序数替换它们，仍然适合GLM
框架。\(\leftrightarrow\) 作为一个例子，我们展示了如何将 DrivAge
转化为一个连续的函数形式。因此，我们根据
修改特征空间\(\chi\)和回归函数\(\lambda(\cdot)\)。我们用以下连续函数替换
7 个分类年龄类别：\(\models\)
\[\mathrm{DrivAge}\mapsto\beta_{l}\:\mathrm{DrivAge}+\beta_{l+1}\log(\mathrm{DrivAge})+\sum_{j=2}^{4}\beta_{l+j}\:(\mathrm{DrivAge})^{j}\:,\]
因此，我们将 7 个分类类别 (涉及来自虚拟编码的 6 个回归参数)
替换为上述具有 5 个回归参数的连续函数形式。

该模型将是我们的基础模型，我们将与该模型比较后续拟合的前馈神经网 络。

\subsubsection{拟合模型}\label{ux62dfux5408ux6a21ux578b}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exec\_time }\OtherTok{\textless{}{-}} \FunctionTok{system.time}\NormalTok{(}
\NormalTok{  glm2 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(ClaimNb }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AreaGLM }\SpecialCharTok{+}\NormalTok{ VehPowerGLM }\SpecialCharTok{+}\NormalTok{ VehAgeGLM }\SpecialCharTok{+}\NormalTok{ BonusMalusGLM }\SpecialCharTok{+}
\NormalTok{                VehBrand }\SpecialCharTok{+}\NormalTok{ VehGas }\SpecialCharTok{+}\NormalTok{ DensityGLM }\SpecialCharTok{+}\NormalTok{ Region }\SpecialCharTok{+}\NormalTok{ DrivAge }\SpecialCharTok{+} 
                \FunctionTok{log}\NormalTok{(DrivAge) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(DrivAge}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(DrivAge}\SpecialCharTok{\^{}}\DecValTok{3}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(DrivAge}\SpecialCharTok{\^{}}\DecValTok{4}\NormalTok{),}
              \AttributeTok{data =}\NormalTok{ train, }\AttributeTok{offset =} \FunctionTok{log}\NormalTok{(Exposure), }\AttributeTok{family =} \FunctionTok{poisson}\NormalTok{())}
\NormalTok{)}
\NormalTok{exec\_time[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  user.self   sys.self    elapsed user.child  sys.child 
##      11.42       0.45      12.11         NA         NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(glm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = ClaimNb ~ AreaGLM + VehPowerGLM + VehAgeGLM + BonusMalusGLM + 
##     VehBrand + VehGas + DensityGLM + Region + DrivAge + log(DrivAge) + 
##     I(DrivAge^2) + I(DrivAge^3) + I(DrivAge^4), family = poisson(), 
##     data = train, offset = log(Exposure))
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(>|z|)    
## (Intercept)    7.653e+01  6.908e+00  11.079  < 2e-16 ***
## AreaGLM        4.681e-02  2.128e-02   2.199 0.027873 *  
## VehPowerGLM5   5.782e-02  2.435e-02   2.374 0.017575 *  
## VehPowerGLM6   9.069e-02  2.389e-02   3.796 0.000147 ***
## VehPowerGLM7   6.610e-02  2.377e-02   2.780 0.005428 ** 
## VehPowerGLM8   9.579e-02  3.381e-02   2.833 0.004605 ** 
## VehPowerGLM9   2.376e-01  2.655e-02   8.949  < 2e-16 ***
## VehAgeGLM1    -1.945e-02  3.419e-02  -0.569 0.569403    
## VehAgeGLM3    -1.840e-01  1.633e-02 -11.272  < 2e-16 ***
## BonusMalusGLM  2.755e-02  4.127e-04  66.755  < 2e-16 ***
## VehBrandB2    -8.200e-03  1.933e-02  -0.424 0.671477    
## VehBrandB3     5.923e-02  2.670e-02   2.219 0.026515 *  
## VehBrandB4     5.602e-02  3.623e-02   1.546 0.122043    
## VehBrandB5     8.473e-02  3.086e-02   2.746 0.006034 ** 
## VehBrandB6     1.326e-02  3.499e-02   0.379 0.704762    
## VehBrandB10    8.286e-03  4.431e-02   0.187 0.851663    
## VehBrandB11    1.860e-01  4.688e-02   3.967 7.27e-05 ***
## VehBrandB12   -2.505e-01  2.442e-02 -10.257  < 2e-16 ***
## VehBrandB13    5.417e-02  4.992e-02   1.085 0.277899    
## VehBrandB14   -1.602e-01  9.794e-02  -1.636 0.101797    
## VehGasRegular -1.569e-01  1.494e-02 -10.500  < 2e-16 ***
## DensityGLM     3.919e-02  1.581e-02   2.478 0.013211 *  
## RegionR11     -9.825e-03  3.111e-02  -0.316 0.752112    
## RegionR21      2.124e-03  1.314e-01   0.016 0.987099    
## RegionR22      1.766e-01  6.414e-02   2.753 0.005906 ** 
## RegionR23     -4.110e-02  7.866e-02  -0.522 0.601323    
## RegionR25     -3.692e-02  5.557e-02  -0.664 0.506376    
## RegionR26      4.565e-02  6.128e-02   0.745 0.456283    
## RegionR31      2.350e-02  4.055e-02   0.579 0.562303    
## RegionR41     -1.508e-01  5.514e-02  -2.735 0.006241 ** 
## RegionR42      2.856e-02  1.168e-01   0.245 0.806763    
## RegionR43     -1.427e-01  1.896e-01  -0.753 0.451682    
## RegionR52      2.480e-02  3.201e-02   0.775 0.438538    
## RegionR53      2.326e-02  2.952e-02   0.788 0.430749    
## RegionR54      3.652e-02  4.265e-02   0.856 0.391850    
## RegionR72      1.099e-01  3.728e-02   2.949 0.003185 ** 
## RegionR73     -1.697e-01  5.944e-02  -2.855 0.004304 ** 
## RegionR74      4.124e-01  7.951e-02   5.187 2.14e-07 ***
## RegionR82      2.234e-01  2.367e-02   9.441  < 2e-16 ***
## RegionR83      1.885e-02  9.388e-02   0.201 0.840865    
## RegionR91     -2.618e-03  3.834e-02  -0.068 0.945544    
## RegionR93      1.446e-01  2.669e-02   5.418 6.04e-08 ***
## RegionR94      1.518e-01  9.800e-02   1.549 0.121306    
## DrivAge        3.874e+00  4.007e-01   9.670  < 2e-16 ***
## log(DrivAge)  -4.661e+01  4.231e+00 -11.018  < 2e-16 ***
## I(DrivAge^2)  -5.549e-02  6.728e-03  -8.248  < 2e-16 ***
## I(DrivAge^3)   4.399e-04  6.360e-05   6.917 4.61e-12 ***
## I(DrivAge^4)  -1.388e-06  2.421e-07  -5.733 9.85e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 136692  on 542330  degrees of freedom
## Residual deviance: 130634  on 542283  degrees of freedom
## AIC: 171306
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

\textbf{练习：}可以通过减少包含的变量来减少参数的数量。这样做并将结果与当前使用的模型进行比较。

\textbf{练习：}从建模的角度来看，可以通过排除不重要的变量来改进这个
glm。

The \texttt{summary()} function for a \texttt{glm} object provides the
statistical tests of significance for every single parameter. However,
with categorical variables the primary interest is to know if a
categorical variables at all is significant. This can be done using the
R function \texttt{drop1}, see its help file for further details. It
performs a Likelihood Ratio Test (LRT) which confirms that only the
p-value for AreaGLM is above 5\%.

\texttt{glm} 对象的
\texttt{summary()}函数为每个参数提供了显著性统计检验。但是，对于分类变量，主要兴趣是了解分类变量是否显著。这可以使用
\texttt{R} 函数
\texttt{dropl}来完成，有关详细信息，请参阅其帮助文件。它执行似然比检验(LRT),确认只有
AreaGLM 的\(p\)值高于 5\%。

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (update\_code }\SpecialCharTok{==} \DecValTok{0}  \SpecialCharTok{\&} \FunctionTok{file.exists.local}\NormalTok{(}\StringTok{"glm\_test.Rdata"}\NormalTok{))\{}
  \FunctionTok{load.file.local}\NormalTok{(}\StringTok{"glm\_test.Rdata"}\NormalTok{)}
\NormalTok{\}}\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{  glm\_test }\OtherTok{\textless{}{-}} \FunctionTok{drop1}\NormalTok{(glm2, }\AttributeTok{test =} \StringTok{"LRT"}\NormalTok{)}
  \FunctionTok{save.file.local}\NormalTok{(glm\_test)}
\NormalTok{\}}
\NormalTok{glm\_test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Single term deletions
## 
## Model:
## ClaimNb ~ AreaGLM + VehPowerGLM + VehAgeGLM + BonusMalusGLM + 
##     VehBrand + VehGas + DensityGLM + Region + DrivAge + log(DrivAge) + 
##     I(DrivAge^2) + I(DrivAge^3) + I(DrivAge^4)
##               Df Deviance    AIC    LRT  Pr(>Chi)    
## <none>             130634 171306                     
## AreaGLM        1   130639 171308    4.8   0.02786 *  
## VehPowerGLM    5   130721 171382   86.7 < 2.2e-16 ***
## VehAgeGLM      2   130763 171431  129.6 < 2.2e-16 ***
## BonusMalusGLM  1   134139 174809 3505.3 < 2.2e-16 ***
## VehBrand      10   130840 171492  206.6 < 2.2e-16 ***
## VehGas         1   130744 171414  110.3 < 2.2e-16 ***
## DensityGLM     1   130640 171310    6.1   0.01316 *  
## Region        21   130836 171466  202.3 < 2.2e-16 ***
## DrivAge        1   130728 171397   93.9 < 2.2e-16 ***
## log(DrivAge)   1   130755 171424  120.7 < 2.2e-16 ***
## I(DrivAge^2)   1   130703 171372   68.9 < 2.2e-16 ***
## I(DrivAge^3)   1   130683 171352   48.7 2.971e-12 ***
## I(DrivAge^4)   1   130667 171337   33.6 6.767e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\textbf{练习:} 考虑使用与 DrivAge 类似的方法来处理另一个特征（例如
BonusMalus）。

\subsubsection{验证模型}\label{ux9a8cux8bc1ux6a21ux578b}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Predictions}
\NormalTok{train}\SpecialCharTok{$}\NormalTok{fitGLM2 }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(glm2)}
\NormalTok{test}\SpecialCharTok{$}\NormalTok{fitGLM2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(glm2, }\AttributeTok{newdata =}\NormalTok{ test, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{fitGLM2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(glm2, }\AttributeTok{newdata =}\NormalTok{ dat2, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# in{-}sample and out{-}of{-}sample losses (in 10\^{}({-}2))}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"100 x Poisson deviance GLM (train): \%s"}\NormalTok{, }\FunctionTok{PoissonDeviance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{fitGLM2, train}\SpecialCharTok{$}\NormalTok{ClaimNb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "100 x Poisson deviance GLM (train): 24.0874788981516"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"100 x Poisson deviance GLM (test): \%s"}\NormalTok{, }\FunctionTok{PoissonDeviance}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitGLM2, test}\SpecialCharTok{$}\NormalTok{ClaimNb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "100 x Poisson deviance GLM (test): 24.1666108887931"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Overall estimated frequency}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"average frequency (test): \%s"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitGLM2) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "average frequency (test): 0.0737"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_cmp }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{model =} \StringTok{"M1: GLM"}\NormalTok{, }\AttributeTok{epochs =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{run\_time =} \FunctionTok{round}\NormalTok{(exec\_time[[}\DecValTok{3}\NormalTok{]], }\DecValTok{0}\NormalTok{), }\AttributeTok{parameters =} \FunctionTok{length}\NormalTok{(}\FunctionTok{coef}\NormalTok{(glm2)),}
             \AttributeTok{in\_sample\_loss =} \FunctionTok{round}\NormalTok{(}\FunctionTok{PoissonDeviance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{fitGLM2, }\FunctionTok{as.vector}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{ClaimNb))), }\DecValTok{4}\NormalTok{),}
             \AttributeTok{out\_sample\_loss =} \FunctionTok{round}\NormalTok{(}\FunctionTok{PoissonDeviance}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitGLM2, }\FunctionTok{as.vector}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{ClaimNb))), }\DecValTok{4}\NormalTok{),}
             \AttributeTok{avg\_freq =} \FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitGLM2) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(df\_cmp)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0933}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1467}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2133}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1200}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
epochs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
run\_time
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
parameters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
in\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
out\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
avg\_freq
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M1: GLM & NA & 12 & 48 & 24.0875 & 24.1666 & 0.0737 \\
\end{longtable}

\subsubsection{校准模型}\label{ux6821ux51c6ux6a21ux578b}

除了使用一些指标来拟合和验证模型之外，检查模型是否在特征空间中得到很好的校准也很重要。例如，可能是模型的整体拟合良好，但存在模型低估和高估索赔频率的区域。后续校准图的目标是确保沿整个特征空间正确拟合。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Area}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"Area"}\NormalTok{, }\StringTok{"frequency by area"}\NormalTok{, }\StringTok{"GLM"}\NormalTok{, }\StringTok{"fitGLM2"}\NormalTok{)}
\CommentTok{\# VehPower}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehPower"}\NormalTok{, }\StringTok{"frequency by vehicle power"}\NormalTok{, }\StringTok{"GLM"}\NormalTok{, }\StringTok{"fitGLM2"}\NormalTok{)}
\CommentTok{\# VehBrand}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehBrand"}\NormalTok{, }\StringTok{"frequency by vehicle brand"}\NormalTok{, }\StringTok{"GLM"}\NormalTok{, }\StringTok{"fitGLM2"}\NormalTok{)}
\CommentTok{\# VehAge}
\NormalTok{p4 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehAge"}\NormalTok{, }\StringTok{"frequency by vehicle age"}\NormalTok{, }\StringTok{"GLM"}\NormalTok{, }\StringTok{"fitGLM2"}\NormalTok{)}

\FunctionTok{grid.arrange}\NormalTok{(p1, p2, p3, p4)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-27-1.pdf}

根据图表，没有检测到任何问题，并且模型似乎已经很好地校准了。

\textbf{练习：} 使用上图中尚未包含的其他变量执行校准。

\subsection{神经网络预处理}\label{ux795eux7ecfux7f51ux7edcux9884ux5904ux7406}

\subsubsection{引入}\label{ux5f15ux5165}

在本章中，我们将解释如何对数据进行预处理以用于神经网络。
它不能以与上面显示的 GLM 相同的方式处理。

我们将强调数据预处理中的几个重要点，它们对于网络的成功应用是必要的。

在网络建模中，特征分量尺度的选择可能会极大地影响预测模型的拟合过
程。因此，数据预处理需要仔细考虑。我们分别处理无序的分类(名义)特征分量和连续(或有序)特征分量。有序的分类特征组件被视为连续的，我们只需用整数替换有序的分类标签。二元分类特征分量由两个二进制标签
0 和 1
编码(对于二进制标签，我们不区分有序和无序分量)。请注意，如果我们选择反对称激活函数，即\(-\phi(x)=\phi(-x)\),我们还可以将二元分类特征分量设置为
\(\pm1/2\),这可以简化优化算法的初始化。

\paragraph{无序（名义）分类特征分量}\label{ux65e0ux5e8fux540dux4e49ux5206ux7c7bux7279ux5f81ux5206ux91cf}

我们需要将(名义)分类特征分量转换为数值。最常用的变换是所谓的\textbf{虚拟变量编码}和
\textbf{one-hot
编码}。两种方法都为分类标签构建二进制表示。对于虚拟编码，
选择一个标签作为参考水平；然后，虚拟编码使用二进制变量来指示特定保单拥有的标签，如果它与参考级别不同。在我们的示例中，我们有两个无序的分类特征组件，即
\texttt{VehBrand} 和 \texttt{Region}。我们使用 \texttt{VehBrand}
作为说明。它有 11
个不同的标签\(\left\{B_1,B_{10},B_{10},B_{11},B_{12},B_{13},B_{14},B_{2},B_{3},B_{4},B_{5},B_{6}\right\}\)。我们选择
B1 作为参考标签，然后虚拟编码提供下面(左)的编码方案。我们观察到 11
个标签被 10维特征向量 \(\{0,1\}^{10}\)替换，其分量总和为0或1。中

\begin{figure}
\centering
\includegraphics{./Figs/Case2/Case2-Fig1.png}
\caption{名义分类变量编码方式}
\end{figure}

与虚拟编码相比，one-hot
编码不选择参考级别，而是为每个标签使用一个指示符。通过这种方式，\texttt{VehBrand}
的 11 个标签被 11 个单位向量替换。虚拟编码和 one-hot
编码之间的主要区别在于前者会导致满秩设计矩阵，而后者不会。这意味着在
one-hot
编码下，参数化存在可识别性问题。在网络建模中，可识别性不太重要，因为我们通常处理过度参数化的非凸优化问题（具有多个同样好的模型/参数化）；另一方面，GLM
中的可识别性是一个重要特征，因为我们通常尝试解决凸优化问题，其中满秩属性对于有效和（唯一）解决方案很重要。
请注意，其他编码方案可用于分类特征组件，例如 Helmert 的对比编码。在经典
GLM
中，编码方案的选择通常不会影响预测，但是，考虑不同的对比可能会改变结果的解释。在网络建模中，编码方案的选择可能会影响预测：通常，我们在网络校准中使用提前停止规则。这种提前停止规则和相应的结果可能取决于任何选择的建模策略，例如分类特征组件的编码方案。
请注意，虚拟编码和 one-hot
编码可能会导致网络中的输入层的维度非常高，并且提供了输入特征的稀疏性。此外，one-hot
编码方案中任意两个标签之间的欧氏距离是相同的。从自然语言处理 (NLP)
中，我们了解到有更有效的方法来表示分类特征组件，即将它们嵌入到低维空间中，以便这些空间中的接近度在回归任务中具有有用的意义。在网络中，这可以通过所谓的嵌入层来实现。

\paragraph{连续特征}\label{ux8fdeux7eedux7279ux5f81}

理论上，如果我们选择一个足够丰富的网络，连续特征组件不需要预处理，因为网络可能会处理不同尺度的特征组件。该陈述具有纯理论价值。在实践中，连续特征组件需要预处理，以便它们都以相似的规模存在，并且它们在这个规模上足够均匀地分布。这个要求的原因是校准算法大多使用梯度下降法（GDM）。这些
GDM
只有在所有组件都以相似的比例存在时才能正常工作，因此所有方向对梯度的贡献都相同。否则，优化算法可能会陷入鞍点或梯度所在的区域（也称为梯度消失问题）。通常，人们使用
\([-1,+1]\)作为通用尺度，因为我们对激活函数的选择集中在该尺度上。

一种流行的转换是所谓的 MinMaxScaler。对于这种转换，我们一次固定 \(x\)
的一个连续特征分量，比如\(x_l\)。分别用 \(m_l\) 和 \(M_l\)
表示\(x_l\)的最小值和最大值，然后 MinMaxScaler 作以下替换：

\[
x_l\mapsto x_l^*=\frac{2(x_l-m_l)}{M_l-m_l}-1\in[-1,1] .
\]

在实践中，可能会发生最小值\(m_l\) 或最大值\(M_l\)
未知的情况。在这种情况下，选择观察数据中特征的相应最小值和/或最大值。对于新特征下的预测，需要保持最初观测数据的原始尺度，即用于模型校准的尺度。

请注意，如果我们有异常值，上述变换可能会导致非常集中的变换特征分量
\(x_l^*\), \(i=1,...,n\) ，因为异常值可能会支配 MinMaxScaler
中的最大值。在这种情况下，应首先通过对数变换或分位数变换对特征分量进行变换，以使它们在实数域上变得更加均匀（和稳健）。

\paragraph{二元分类特征}\label{ux4e8cux5143ux5206ux7c7bux7279ux5f81}

机器学习
文献中的二元分类特征（例如性别）通常嵌入到到更高维空间中。然而，我们认为这没有意义。因此，我们建议将二元分类特征设置为\(\pm 1/2\)。

\paragraph{小结}\label{ux5c0fux7ed3}

根据经验，可以将其表述如下：

\begin{itemize}
\tightlist
\item
  连续特征分量\(\Rightarrow\)缩放到 \([-1,+1]\) (if no outliers)
\item
  二分类特征 \(\Rightarrow\) 设置为 \(\{-1/2,+1/2 \}\)
\item
  多分类特征:

  \begin{itemize}
  \tightlist
  \item
    使其数字化 \(\Rightarrow\) 缩放到 \([-1,+1]\)
  \item
    One-hot 编码 \(\Rightarrow\) 不缩放
  \item
    虚拟变量 编码 \(\Rightarrow\) 不缩放
  \item
    嵌入层 \(\Rightarrow\) 使其数字化且无缩放
  \end{itemize}
\end{itemize}

\subsubsection{预处理函数}\label{ux9884ux5904ux7406ux51fdux6570}

在我们的例子中，我们对特征VehBrand和Region使用虚拟编码。我们对Area使用MinMaxScaler（在将\(\{A,...,F\}\)映射到\(\{1,...,6\}\)之后），对VehPower、VehAge（车龄上限为20）、DrivAge（驾驶员年龄上限为90岁）、BonusMalus（奖惩级别上限为150）和Density（进行对数变换）。我们将VehGas转换为\(\pm 1/2\)，并保持风险暴露Exposure
\(\in (0,1]\)不变。

以下是对应的预处理函数：

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MinMax scaler}
\NormalTok{preprocess\_minmax }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(varData) \{}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(varData)}
  \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (X }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(X)) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{max}\NormalTok{(X) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(X)) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{\}}

\CommentTok{\# Dummy coding }
\NormalTok{preprocess\_catdummy }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, varName, prefix) \{}
\NormalTok{  varData }\OtherTok{\textless{}{-}}\NormalTok{ data[[varName]]}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(varData)}
\NormalTok{  n0 }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(X))}
\NormalTok{  n1 }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{:}\NormalTok{n0}
\NormalTok{  addCols }\OtherTok{\textless{}{-}}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(n1, }\ControlFlowTok{function}\NormalTok{(x, y) \{}\FunctionTok{as.integer}\NormalTok{(y }\SpecialCharTok{==}\NormalTok{ x)\}, }\AttributeTok{y =}\NormalTok{ X) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rlang}\SpecialCharTok{::}\FunctionTok{set\_names}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(prefix, n1))}
  \FunctionTok{cbind}\NormalTok{(data, addCols)}
\NormalTok{\}}

\CommentTok{\# Feature pre{-}processing using MinMax Scaler and Dummy Coding}
\NormalTok{preprocess\_features }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate\_at}\NormalTok{(}
      \FunctionTok{c}\NormalTok{(}\AttributeTok{AreaX =} \StringTok{"Area"}\NormalTok{, }\AttributeTok{VehPowerX =} \StringTok{"VehPower"}\NormalTok{, }\AttributeTok{VehAgeX =} \StringTok{"VehAge"}\NormalTok{,}
        \AttributeTok{DrivAgeX =} \StringTok{"DrivAge"}\NormalTok{, }\AttributeTok{BonusMalusX =} \StringTok{"BonusMalus"}\NormalTok{, }\AttributeTok{DensityX =} \StringTok{"Density"}\NormalTok{),}
\NormalTok{      preprocess\_minmax}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
      \AttributeTok{VehGasX =} \FunctionTok{as.integer}\NormalTok{(VehGas) }\SpecialCharTok{{-}} \FloatTok{1.5}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{preprocess\_catdummy}\NormalTok{(}\StringTok{"VehBrand"}\NormalTok{, }\StringTok{"Br"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{preprocess\_catdummy}\NormalTok{(}\StringTok{"Region"}\NormalTok{, }\StringTok{"R"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsubsection{执行预处理}\label{ux6267ux884cux9884ux5904ux7406}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat2 }\OtherTok{\textless{}{-}} \FunctionTok{preprocess\_features}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\subsubsection{检查预处理数据}\label{ux68c0ux67e5ux9884ux5904ux7406ux6570ux636e}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(dat2))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0194}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0290}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0290}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0226}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0258}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0355}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0290}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0258}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0258}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0226}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0355}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0258}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0290}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0323}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0194}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0355}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0258}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0355}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0387}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0355}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0258}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0097}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0097}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0097}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0097}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0097}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0097}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0097}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0097}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 104\tabcolsep) * \real{0.0129}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
IDpol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Exposure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Area
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
VehPower
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
VehAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DrivAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BonusMalus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VehBrand
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
VehGas
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Density
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ClaimTotal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ClaimNb
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
group\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fitGLM2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AreaX
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
VehPowerX
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
VehAgeX
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DrivAgeX
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BonusMalusX
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DensityX
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
VehGasX
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br10
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Br11
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R10
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R11
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R12
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R13
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R14
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R15
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R16
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R17
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R18
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R19
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R20
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R21
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
R22
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0.10 & D & 5 & 0 & 55 & 50 & B12 & Regular & 7.10 & R82 & 0 & 0 & 1
& 0.0058255 & 0.2 & -0.8181818 & -1.0 & 0.0277778 & -1 & 0.3921569 & 0.5
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
3 & 0.77 & D & 5 & 0 & 55 & 50 & B12 & Regular & 7.10 & R82 & 0 & 0 & 1
& 0.0448565 & 0.2 & -0.8181818 & -1.0 & 0.0277778 & -1 & 0.3921569 & 0.5
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
5 & 0.75 & B & 6 & 2 & 52 & 50 & B12 & Diesel & 3.99 & R22 & 0 & 0 & 2 &
0.0423253 & -0.6 & -0.6363636 & -0.8 & -0.0555556 & -1 & -0.2176471 &
-0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
10 & 0.09 & B & 7 & 0 & 46 & 50 & B12 & Diesel & 4.33 & R72 & 0 & 0 & 3
& 0.0045857 & -0.6 & -0.4545455 & -1.0 & -0.2222222 & -1 & -0.1509804 &
-0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
11 & 0.84 & B & 7 & 0 & 46 & 50 & B12 & Diesel & 4.33 & R72 & 0 & 0 & 3
& 0.0427997 & -0.6 & -0.4545455 & -1.0 & -0.2222222 & -1 & -0.1509804 &
-0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
13 & 0.52 & E & 6 & 2 & 38 & 50 & B12 & Regular & 8.01 & R31 & 0 & 0 & 4
& 0.0245961 & 0.6 & -0.6363636 & -0.8 & -0.4444444 & -1 & 0.5705882 &
0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(dat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    678007 obs. of  53 variables:
##  $ IDpol      : num  1 3 5 10 11 13 15 17 18 21 ...
##  $ Exposure   : num  0.1 0.77 0.75 0.09 0.84 0.52 0.45 0.27 0.71 0.15 ...
##  $ Area       : Factor w/ 6 levels "A","B","C","D",..: 4 4 2 2 2 5 5 3 3 2 ...
##  $ VehPower   : num  5 5 6 7 7 6 6 7 7 7 ...
##  $ VehAge     : num  0 0 2 0 0 2 2 0 0 0 ...
##  $ DrivAge    : num  55 55 52 46 46 38 38 33 33 41 ...
##  $ BonusMalus : num  50 50 50 50 50 50 50 68 68 50 ...
##  $ VehBrand   : Factor w/ 11 levels "B1","B2","B3",..: 9 9 9 9 9 9 9 9 9 9 ...
##  $ VehGas     : Factor w/ 2 levels "Diesel","Regular": 2 2 1 1 1 2 2 1 1 1 ...
##  $ Density    : num  7.1 7.1 3.99 4.33 4.33 8.01 8.01 4.92 4.92 4.09 ...
##  $ Region     : Factor w/ 22 levels "R11","R21","R22",..: 18 18 3 15 15 8 8 20 20 12 ...
##  $ ClaimTotal : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ ClaimNb    : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ group_id   : int  1 1 2 3 3 4 4 5 5 6 ...
##  $ fitGLM2    : num  0.00583 0.04486 0.04233 0.00459 0.0428 ...
##  $ AreaX      : num  0.2 0.2 -0.6 -0.6 -0.6 0.6 0.6 -0.2 -0.2 -0.6 ...
##  $ VehPowerX  : num  -0.818 -0.818 -0.636 -0.455 -0.455 ...
##  $ VehAgeX    : num  -1 -1 -0.8 -1 -1 -0.8 -0.8 -1 -1 -1 ...
##  $ DrivAgeX   : num  0.0278 0.0278 -0.0556 -0.2222 -0.2222 ...
##  $ BonusMalusX: num  -1 -1 -1 -1 -1 -1 -1 -0.64 -0.64 -1 ...
##  $ DensityX   : num  0.392 0.392 -0.218 -0.151 -0.151 ...
##  $ VehGasX    : num  0.5 0.5 -0.5 -0.5 -0.5 0.5 0.5 -0.5 -0.5 -0.5 ...
##  $ Br2        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Br3        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Br4        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Br5        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Br6        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Br7        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Br8        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Br9        : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ Br10       : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Br11       : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R2         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R3         : int  0 0 1 0 0 0 0 0 0 0 ...
##  $ R4         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R5         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R6         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R7         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R8         : int  0 0 0 0 0 1 1 0 0 0 ...
##  $ R9         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R10        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R11        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R12        : int  0 0 0 0 0 0 0 0 0 1 ...
##  $ R13        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R14        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R15        : int  0 0 0 1 1 0 0 0 0 0 ...
##  $ R16        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R17        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R18        : int  1 1 0 0 0 0 0 0 0 0 ...
##  $ R19        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R20        : int  0 0 0 0 0 0 0 1 1 0 ...
##  $ R21        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ R22        : int  0 0 0 0 0 0 0 0 0 0 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(dat2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      IDpol            Exposure        Area          VehPower     
##  Min.   :      1   Min.   :0.002732   A:103957   Min.   : 4.000  
##  1st Qu.:1157948   1st Qu.:0.180000   B: 75459   1st Qu.: 5.000  
##  Median :2272153   Median :0.490000   C:191880   Median : 6.000  
##  Mean   :2621857   Mean   :0.528547   D:151590   Mean   : 6.455  
##  3rd Qu.:4046278   3rd Qu.:0.990000   E:137167   3rd Qu.: 7.000  
##  Max.   :6114330   Max.   :1.000000   F: 17954   Max.   :15.000  
##                                                                  
##      VehAge          DrivAge       BonusMalus        VehBrand     
##  Min.   : 0.000   Min.   :18.0   Min.   : 50.00   B12    :166024  
##  1st Qu.: 2.000   1st Qu.:34.0   1st Qu.: 50.00   B1     :162730  
##  Median : 6.000   Median :44.0   Median : 50.00   B2     :159861  
##  Mean   : 6.976   Mean   :45.5   Mean   : 59.76   B3     : 53395  
##  3rd Qu.:11.000   3rd Qu.:55.0   3rd Qu.: 64.00   B5     : 34753  
##  Max.   :20.000   Max.   :90.0   Max.   :150.00   B6     : 28548  
##                                                   (Other): 72696  
##      VehGas          Density           Region         ClaimTotal     
##  Diesel :332136   Min.   : 0.000   R24    :160601   Min.   :      0  
##  Regular:345871   1st Qu.: 4.520   R82    : 84752   1st Qu.:      0  
##                   Median : 5.970   R93    : 79315   Median :      0  
##                   Mean   : 5.982   R11    : 69791   Mean   :     88  
##                   3rd Qu.: 7.410   R53    : 42122   3rd Qu.:      0  
##                   Max.   :10.200   R52    : 38751   Max.   :4075401  
##                                    (Other):202675                    
##     ClaimNb           group_id         fitGLM2              AreaX         
##  Min.   :0.00000   Min.   :     1   Min.   :0.0000632   Min.   :-1.00000  
##  1st Qu.:0.00000   1st Qu.:149319   1st Qu.:0.0118361   1st Qu.:-0.60000  
##  Median :0.00000   Median :273211   Median :0.0329438   Median :-0.20000  
##  Mean   :0.03891   Mean   :275320   Mean   :0.0389177   Mean   :-0.08412  
##  3rd Qu.:0.00000   3rd Qu.:404072   3rd Qu.:0.0545373   3rd Qu.: 0.20000  
##  Max.   :4.00000   Max.   :534079   Max.   :2.0223469   Max.   : 1.00000  
##                                                                           
##    VehPowerX          VehAgeX           DrivAgeX         BonusMalusX     
##  Min.   :-1.0000   Min.   :-1.0000   Min.   :-1.00000   Min.   :-1.0000  
##  1st Qu.:-0.8182   1st Qu.:-0.8000   1st Qu.:-0.55556   1st Qu.:-1.0000  
##  Median :-0.6364   Median :-0.4000   Median :-0.27778   Median :-1.0000  
##  Mean   :-0.5537   Mean   :-0.3024   Mean   :-0.23620   Mean   :-0.8049  
##  3rd Qu.:-0.4545   3rd Qu.: 0.1000   3rd Qu.: 0.02778   3rd Qu.:-0.7200  
##  Max.   : 1.0000   Max.   : 1.0000   Max.   : 1.00000   Max.   : 1.0000  
##                                                                          
##     DensityX          VehGasX              Br2              Br3         
##  Min.   :-1.0000   Min.   :-0.50000   Min.   :0.0000   Min.   :0.00000  
##  1st Qu.:-0.1137   1st Qu.:-0.50000   1st Qu.:0.0000   1st Qu.:0.00000  
##  Median : 0.1706   Median : 0.50000   Median :0.0000   Median :0.00000  
##  Mean   : 0.1729   Mean   : 0.01013   Mean   :0.2358   Mean   :0.07875  
##  3rd Qu.: 0.4529   3rd Qu.: 0.50000   3rd Qu.:0.0000   3rd Qu.:0.00000  
##  Max.   : 1.0000   Max.   : 0.50000   Max.   :1.0000   Max.   :1.00000  
##                                                                         
##       Br4               Br5               Br6               Br7         
##  Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  
##  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  
##  Median :0.00000   Median :0.00000   Median :0.00000   Median :0.00000  
##  Mean   :0.03714   Mean   :0.05126   Mean   :0.04211   Mean   :0.02612  
##  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  
##  Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  
##                                                                         
##       Br8               Br9              Br10              Br11         
##  Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.000000  
##  1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.000000  
##  Median :0.00000   Median :0.0000   Median :0.00000   Median :0.000000  
##  Mean   :0.02004   Mean   :0.2449   Mean   :0.01796   Mean   :0.005969  
##  3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.000000  
##  Max.   :1.00000   Max.   :1.0000   Max.   :1.00000   Max.   :1.000000  
##                                                                         
##        R2                 R3                R4                R5        
##  Min.   :0.000000   Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  
##  1st Qu.:0.000000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000  
##  Median :0.000000   Median :0.00000   Median :0.00000   Median :0.0000  
##  Mean   :0.004463   Mean   :0.01179   Mean   :0.01296   Mean   :0.2369  
##  3rd Qu.:0.000000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000  
##  Max.   :1.000000   Max.   :1.00000   Max.   :1.00000   Max.   :1.0000  
##                                                                         
##        R6                R7                R8                R9         
##  Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  
##  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  
##  Median :0.00000   Median :0.00000   Median :0.00000   Median :0.00000  
##  Mean   :0.01607   Mean   :0.01547   Mean   :0.04024   Mean   :0.01916  
##  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  
##  Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  
##                                                                         
##       R10                R11                R12               R13         
##  Min.   :0.000000   Min.   :0.000000   Min.   :0.00000   Min.   :0.00000  
##  1st Qu.:0.000000   1st Qu.:0.000000   1st Qu.:0.00000   1st Qu.:0.00000  
##  Median :0.000000   Median :0.000000   Median :0.00000   Median :0.00000  
##  Mean   :0.003245   Mean   :0.001956   Mean   :0.05715   Mean   :0.06213  
##  3rd Qu.:0.000000   3rd Qu.:0.000000   3rd Qu.:0.00000   3rd Qu.:0.00000  
##  Max.   :1.000000   Max.   :1.000000   Max.   :1.00000   Max.   :1.00000  
##                                                                           
##       R14               R15               R16               R17          
##  Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000000  
##  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000000  
##  Median :0.00000   Median :0.00000   Median :0.00000   Median :0.000000  
##  Mean   :0.02809   Mean   :0.04621   Mean   :0.02528   Mean   :0.006736  
##  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000000  
##  Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.000000  
##                                                                          
##       R18             R19                R20              R21       
##  Min.   :0.000   Min.   :0.000000   Min.   :0.0000   Min.   :0.000  
##  1st Qu.:0.000   1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:0.000  
##  Median :0.000   Median :0.000000   Median :0.0000   Median :0.000  
##  Mean   :0.125   Mean   :0.007798   Mean   :0.0528   Mean   :0.117  
##  3rd Qu.:0.000   3rd Qu.:0.000000   3rd Qu.:0.0000   3rd Qu.:0.000  
##  Max.   :1.000   Max.   :1.000000   Max.   :1.0000   Max.   :1.000  
##                                                                     
##       R22          
##  Min.   :0.000000  
##  1st Qu.:0.000000  
##  Median :0.000000  
##  Mean   :0.006661  
##  3rd Qu.:0.000000  
##  Max.   :1.000000  
## 
\end{verbatim}

\subsubsection{划分训练和测试数据集}\label{ux5212ux5206ux8badux7ec3ux548cux6d4bux8bd5ux6570ux636eux96c6-1}

首先，我们将数据集拆分为训练和测试。由于保单中潜在的行分组，我们不能只进行随机拆分。为此，我们使用
\texttt{splitTools} 包中的函数 \texttt{partition(...)}。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ind }\OtherTok{\textless{}{-}} \FunctionTok{partition}\NormalTok{(dat2[[}\StringTok{"group\_id"}\NormalTok{]], }\AttributeTok{p =} \FunctionTok{c}\NormalTok{(}\AttributeTok{train =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{test =} \FloatTok{0.2}\NormalTok{), }
                 \AttributeTok{seed =}\NormalTok{ seed, }\AttributeTok{type =} \StringTok{"grouped"}\NormalTok{)}
\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ dat2[ind}\SpecialCharTok{$}\NormalTok{train, ]}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ dat2[ind}\SpecialCharTok{$}\NormalTok{test, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# size of train/test}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of observations (train): \%s"}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(train))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of observations (train): 542331"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of observations (test): \%s"}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of observations (test): 135676"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Claims frequency of train/test}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Empirical frequency (train): \%s"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{ClaimNb) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Empirical frequency (train): 0.0736"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Empirical frequency (test): \%s"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{ClaimNb) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Empirical frequency (test): 0.0736"
\end{verbatim}

\subsubsection{常见的神经网络规范}\label{ux5e38ux89c1ux7684ux795eux7ecfux7f51ux7edcux89c4ux8303}

在本节中，我们定义对象和参数，这些对象和参数将用于所有后续考虑的神经网络，独立于它们的网络结构。

我们需要定义预处理数据集中
\texttt{dat2}中的哪些组件被用作输入特性。由于我们在原始特征的基础上添加了适合神经网络的预处理特征，所以我们只需要使用相关的特征即可。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the feature space}
\NormalTok{col\_start }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(dat) }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{col\_end }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(dat2)}
\NormalTok{features }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(col\_start}\SpecialCharTok{:}\NormalTok{col\_end)  }\CommentTok{\# select features, be careful if pre{-}processing changes}
\FunctionTok{print}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(train[, features]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "AreaX"       "VehPowerX"   "VehAgeX"     "DrivAgeX"    "BonusMalusX"
##  [6] "DensityX"    "VehGasX"     "Br2"         "Br3"         "Br4"        
## [11] "Br5"         "Br6"         "Br7"         "Br8"         "Br9"        
## [16] "Br10"        "Br11"        "R2"          "R3"          "R4"         
## [21] "R5"          "R6"          "R7"          "R8"          "R9"         
## [26] "R10"         "R11"         "R12"         "R13"         "R14"        
## [31] "R15"         "R16"         "R17"         "R18"         "R19"        
## [36] "R20"         "R21"         "R22"
\end{verbatim}

keras
的输入要求训练和测试数据是矩阵格式，包括矩阵中使用的所有特征并且已经正确预处理。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# feature matrix}
\NormalTok{Xtrain }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(train[, features])  }\CommentTok{\# design matrix training sample}
\NormalTok{Xtest }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(test[, features])    }\CommentTok{\# design matrix test sample}
\end{Highlighting}
\end{Shaded}

\subsection{设计神经网络}\label{ux8bbeux8ba1ux795eux7ecfux7f51ux7edc}

特定网络架构的选择及其校准涉及许多步骤。
在这些步骤的每一个中，建模者都必须做出某些决定，并且可能需要对这些决定中的每一个进行多次修改，以便获得最好的（或者更恰当地说，是一个良好的）预测模型。在本节中，我们对以下明确使用的内容进行简短说明。

本教程涵盖了特定网络架构的选择及其校准。在这些步骤的每一个中，建模者都必须做出某些决定，并且可能需要对这些决定中的每一个进行多次修改，以便获得最好的（或者更恰当地说，是一个良好的）预测模型。这些选择包括：

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{(\alph{enumi})}
  \tightlist
  \item
    数据清洗和数据预处理；
  \end{enumerate}
\item
  (b)模型校准的损失函数(目标函数)和性能度量指标的选择；
\item
  \begin{enumerate}
  \def\labelenumi{(\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    隐藏层的数量\$K;
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    隐藏层中神经元的数量\(q_1, \ldots , q_K\) ;
  \end{enumerate}
\item
  (e)激活函数\(\phi\)的选择；
\item
  \begin{enumerate}
  \def\labelenumi{(\alph{enumi})}
  \setcounter{enumi}{5}
  \tightlist
  \item
    用于校准的优化算法，其中可能包括进一步的选择：``
  \end{enumerate}

  \begin{itemize}
  \item
    \begin{enumerate}
    \def\labelenumi{(\roman{enumi})}
    \tightlist
    \item
      初始化算法；
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{(\roman{enumi})}
    \setcounter{enumi}{1}
    \tightlist
    \item
      随机(小)批数据；
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{(\roman{enumi})}
    \setcounter{enumi}{2}
    \tightlist
    \item
      早停，迭代次数，epochs 数量等；
    \end{enumerate}
  \item
    (iv)参数如学习速率，动量参数等；
  \end{itemize}
\item
  (g)标准化层，dropout 率；
\item
  (h)正则化，如 LASSO 或岭回归等；
\end{itemize}

这些选择对应于通常在统计应用程序中执行的建模周期，上面没有提到最终验证步骤。

\subsubsection{梯度下降方法}\label{ux68afux5ea6ux4e0bux964dux65b9ux6cd5}

有几种优化器可用于找到神经网络的解决方案，简短描述如下：

\begin{itemize}
\tightlist
\item
  随机梯度下降法，称为``sgd''，可以通过使用最佳学习率、基于动量的改进、Nesterov
  加速和最佳批次来微调收敛速度。``随机''梯度意味着与（最陡的）梯度下降相比，我们在随机子样本上探索局部最优步骤；
\item
  ``adagrad''选择在梯度的所有方向上不同的学习率，并考虑梯度的方向大小（``ada''代表
  adapted）；
\item
  ``adadelta''是``adagrad''的改进版本，克服了后者的一些缺陷，如对超参数的敏感性；
\item
  ``rmsprop''是另一种克服``adagrad''缺陷的方法（``rmsprop''代表root mean
  square propagation）； for root mean square propagation);
\item
  ``adam''代表自适应矩估计，类似于``adagrad''，它根据由``2-范数''测量的过去梯度诱导的动量，搜索定向最优学习率；
\item
  ``adamax''以类似于``adam''的方式考虑最优学习率，但基于 \(l_1\) 范数；
\item
  ``nadam''是``adam''的 Nesterov 加速版本。
\end{itemize}

The study shows that \texttt{nadam} is a good candidate to be used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# available optimizers for keras}
\CommentTok{\# https://keras.io/optimizers/}
\NormalTok{optimizers }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}sgd\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}adagrad\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}adadelta\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}rmsprop\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}adam\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}adamax\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}nadam\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsubsection{Epochs 和 batches}\label{epochs-ux548c-batches}

Epochs 表示我们遍历整个学习数据\(\mathcal{D}\)的次数，batch size
表示在每个梯度下降方法
(GDM)步骤中考虑的子样本的大小。因此，如果批大小等于观察数\(n\),我们在一个
epoch 中执行一个 GDM 步骤；如果批大小等于 1,那么我们在一个 epoch
中执行\(n\)个 GDM 步骤，直到我们看到整个学习数据\(\mathcal{D}\)
。请注意，
大数据需要较小的批次，因为如果我们有很多观察结果，同时有效地计算所有数据的梯度是不可行的。因此，我们在
GDM
的应用中将整个数据随机划分为(小)批次。请注意，如果我们处理大数据，这种数据分区特别重要，因为它允许我们按顺序探索数据。

具体来说，对于最大的批大小\(n\),我们可以在一个 epoch 内执行一个 GDM
步骤，对于批大小\(k\),我们可以在一个时期内执行 \(n/k\) 个 GDM
步骤。对于最大的批大小，我们需要计算整个数据\(\mathcal{D}\)的梯度。当然，后者要快得多，但另一方面，我们需要计算\(n/k\)个梯度以遍历整个数据(在一个
epoch 内)。``

将数据\(\mathcal{D}\)划分为批次是随机完成的，并且可能会发生多个潜在异常值位于同一批次中的情况。如果选择的批大小较小且预期频率较低(类不平衡问题)
时，尤其会发生这种情况。

\subsubsection{初始化}\label{ux521dux59cbux5316}

将初始网络调整到正确水平的一个简单方法是将同质模型嵌入到神经网络中。这可以通过将神经元的输出权值设为零，并将输出截距初始化为同质模型来实现。这是随后由定义权重\(\mathtt{weights}\)的代码部分获得的。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# homogeneous model (train)}
\NormalTok{lambda\_hom }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{ClaimNb) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure)}
\end{Highlighting}
\end{Shaded}

\subsection{激活函数}\label{ux6fc0ux6d3bux51fdux6570}

接下来我们讨论激活函数 \(\phi(\cdot)\) 的选择。激活函数的典型选择是：

\(\phi:\mathbb{R}\to\mathbb{R}\)
是一个(非线性)激活函数,它模拟了神经元中激活的强度。通常,会做出以下四种选择之一:

\begin{itemize}
\item
  \(\phi(x)=\frac{1}{1+e^{-x}}:\) sigmoid 激活函数
\item
  \(\phi(x)=\tanh(x):\) 双曲正切激活函数
\item
  \(\phi(x)=1_{\{x\geq0\}}:\)阶跃激活函数
\item
  \(\phi(x)=x1_{\{x\geq0\}}:\) ReLU 激活函数
\end{itemize}

激活函数的特定选择可能很重要：如果我们选择双曲正切激活，深度网络的校准可能会稍微简单一些，因为这将保证所有隐藏神经元都在
\((-1,+1)\) 中，这是下一层神经元的主要激活域。

阶跃函数激活对于理论考虑很有用。从实际的角度来看，它的用处不大，因为它不可微分且难以校准。此外，不连续性还意味着相邻特征分量可能具有相当不同的回归函数响应：如果阶跃函数在驾驶员年龄
48 岁和 49
岁之间跳跃，那么这两个驾驶员可能具有相当不同的保险费。由于这些原因，我们在这里不进行阶跃函数激活。

我们注意到 ReLU
激活函数经常导致深度网络激活的稀疏性，因为一些神经元在整个输入中保持未激活状态。这种效果可能是需要的，因为它降低了回归模型的复杂性，但它也可能是不希望的副作用，因为它可能会因为更多的梯度消失而增加模型校准的难度。此外，ReLU
可能会导致神经元中任意大的激活，因为它是一个无界的激活函数，这可能是不想要的效果，因为它可能需要将激活重新缩放到原点周围的主域。

\subsection{模型 2：浅层普通神经网络
(shNN)}\label{ux6a21ux578b-2ux6d45ux5c42ux666eux901aux795eux7ecfux7f51ux7edc-shnn}

让我们首先拟合最简单的前馈神经网络，即所谓的浅层普通神经网络。在拟合和指定一个神经网络之前，我们强烈建议绘制它。这有助于使用
\texttt{keras} 函数。

我们选择一个\(K=1\) 个隐藏层 and \(q_1=20\) *个神经元的网络，如下所示：

\begin{figure}
\centering
\includegraphics{./Figs/Case2/Case2-Fig2.png}
\caption{单层神经网络结构图}
\end{figure}

输入层的维度由选定的输入特征维度确定（见上图）。

\subsubsection{定义}\label{ux5b9aux4e49-1}

下面我们定义网络参数。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define network and load pre{-}specified weights}
\NormalTok{q0 }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(features)                  }\CommentTok{\# dimension of features}
\NormalTok{q1 }\OtherTok{\textless{}{-}} \DecValTok{20}                                \CommentTok{\# number of hidden neurons in hidden layer}

\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Neural network with K=1 hidden layer"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Neural network with K=1 hidden layer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Input feature dimension: q0 = \%s"}\NormalTok{, q0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Input feature dimension: q0 = 38"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of hidden neurons: q1 = \%s"}\NormalTok{, q1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of hidden neurons: q1 = 20"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Output dimension: \%s"}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Output dimension: 1"
\end{verbatim}

下面我们定义具有 \(q_1\)
个隐藏神经元和双曲正切激活函数的前馈浅层网络，由于 Poisson
GLM，输出具有指数激活函数。

风险暴露数作为偏移量包含在内，我们使用同质模型来初始化输出。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Design  }\OtherTok{\textless{}{-}} \FunctionTok{layer\_input}\NormalTok{(}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(q0), }\AttributeTok{dtype =} \StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}Design\textquotesingle{}}\NormalTok{) }
\NormalTok{LogVol  }\OtherTok{\textless{}{-}} \FunctionTok{layer\_input}\NormalTok{(}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{), }\AttributeTok{dtype =} \StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}LogVol\textquotesingle{}}\NormalTok{)}

\NormalTok{Network }\OtherTok{\textless{}{-}}\NormalTok{ Design }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =}\NormalTok{ q1, }\AttributeTok{activation =} \StringTok{\textquotesingle{}tanh\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}layer1\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}linear\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}Network\textquotesingle{}}\NormalTok{,}
              \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(}\FunctionTok{array}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(q1, }\DecValTok{1}\NormalTok{)), }\FunctionTok{array}\NormalTok{(}\FunctionTok{log}\NormalTok{(lambda\_hom), }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))))}

\CommentTok{\# Fix this code by lhn}
\NormalTok{Response }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(Network, LogVol) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_add}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =}\NormalTok{ k\_exp, }\AttributeTok{name =} \StringTok{\textquotesingle{}Response\textquotesingle{}}\NormalTok{, }\AttributeTok{trainable =} \ConstantTok{FALSE}\NormalTok{,}
              \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(}\FunctionTok{array}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)), }\FunctionTok{array}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))))}

\NormalTok{model\_sh }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model}\NormalTok{(}\AttributeTok{inputs =} \FunctionTok{c}\NormalTok{(Design, LogVol), }\AttributeTok{outputs =} \FunctionTok{c}\NormalTok{(Response))}
\end{Highlighting}
\end{Shaded}

\subsubsection{编译模型}\label{ux7f16ux8bd1ux6a21ux578b}

让我们编译模型，使用泊松偏差损失函数作为目标函数，使用 \texttt{nadam}
作为优化器，我们提供了网络结构的摘要。更多细节，我们参考
\texttt{compile}
的\href{https://keras.rstudio.com/reference/compile.html}{帮助文件}。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_sh }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{loss =} \StringTok{\textquotesingle{}poisson\textquotesingle{}}\NormalTok{,}
  \AttributeTok{optimizer =}\NormalTok{ optimizers[}\DecValTok{7}\NormalTok{]}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(model\_sh)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Model: "model"
## ________________________________________________________________________________
##  Layer (type)       Output Shape         Para   Connected to         Trainable  
##                                          m #                                    
## ================================================================================
##  Design (InputLaye  [(None, 38)]         0      []                   Y          
##  r)                                                                             
##  layer1 (Dense)     (None, 20)           780    ['Design[0][0]']     Y          
##  Network (Dense)    (None, 1)            21     ['layer1[0][0]']     Y          
##  LogVol (InputLaye  [(None, 1)]          0      []                   Y          
##  r)                                                                             
##  add (Add)          (None, 1)            0      ['Network[0][0]',    Y          
##                                                  'LogVol[0][0]']                
##  Response (Dense)   (None, 1)            2      ['add[0][0]']        N          
## ================================================================================
## Total params: 803 (3.14 KB)
## Trainable params: 801 (3.13 KB)
## Non-trainable params: 2 (8.00 Byte)
## ________________________________________________________________________________
\end{verbatim}

此摘要对于很好地理解拟合模型至关重要。它包含参数的总数，并显示风险暴露数作为偏移量是如何包括在内的（没有训练相应的权重）。

\subsubsection{拟合模型}\label{ux62dfux5408ux6a21ux578b-1}

为了拟合 \texttt{keras} 模型及其参数，我们在此处参考 \texttt{fit}
的\href{https://keras.rstudio.com/reference/fit.html}{帮助文件}，可以找到有关
\texttt{validation\_split} 和 \texttt{verbose} 参数的详细信息。

如果validation\_split\textgreater0，则将训练集进一步细分为新的训练集和验证集。新的训练集用于拟合模型，验证集用于（样本外）验证。我们强调，验证集与测试集是分开选择的，因为后面的数据可能稍后将用于选择最佳模型（例如，如果我们需要在几个网络之间做出决定）。

在 validation\_split=0.2 的情况下，我们将学习数据按照 8:2
分成训练集和验证集。我们在训练集上拟合网络，并在验证集上进行样本外验证。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set hyperparameters}
\NormalTok{epochs }\OtherTok{\textless{}{-}} \DecValTok{300}
\NormalTok{batch\_size }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{validation\_split }\OtherTok{\textless{}{-}} \FloatTok{0.2}  \CommentTok{\# set to \textgreater{}0 to see train/validation loss in plot(fit)}
\NormalTok{verbose }\OtherTok{\textless{}{-}} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# expected run{-}time on Renku 8GB environment around 70 seconds}
\NormalTok{exec\_time }\OtherTok{\textless{}{-}} \FunctionTok{system.time}\NormalTok{(}
  \ControlFlowTok{if}\NormalTok{ (update\_code }\SpecialCharTok{==} \DecValTok{0}  \SpecialCharTok{\&} \FunctionTok{file.exists.local}\NormalTok{(}\StringTok{"fit\_model\_sh.Rdata"}\NormalTok{))\{}
    \FunctionTok{load.file.local}\NormalTok{(}\StringTok{"fit\_model\_sh.Rdata"}\NormalTok{)}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{    fit\_model\_sh }\OtherTok{\textless{}{-}}\NormalTok{ model\_sh }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{fit}\NormalTok{(}
      \FunctionTok{list}\NormalTok{(Xtrain, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure))), }\FunctionTok{as.matrix}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{ClaimNb),}
      \AttributeTok{epochs =}\NormalTok{ epochs,}
      \AttributeTok{batch\_size =}\NormalTok{ batch\_size,}
      \AttributeTok{validation\_split =}\NormalTok{ validation\_split,}
      \AttributeTok{verbose =}\NormalTok{ verbose}
\NormalTok{    )}
    \FunctionTok{save.file.local}\NormalTok{(fit\_model\_sh)}
\NormalTok{  \}}
\NormalTok{)}
\NormalTok{fit }\OtherTok{\textless{}{-}}\NormalTok{ fit\_model\_sh}
\NormalTok{exec\_time[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  user.self   sys.self    elapsed user.child  sys.child 
##       0.00       0.00       0.01         NA         NA
\end{verbatim}

让我们举例说明梯度下降算法中损失的减少。我们提供以下两张图表：

\begin{itemize}
\tightlist
\item
  第一个：根据参数
  validation\_split，您会看到一条曲线或两条曲线（训练集和验证集）；
\item
  第二个：仅在 validation\_split \textgreater{} 0 时显示。
\end{itemize}

这些图有助于确定最佳的 epochs 数量。

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-44-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_loss}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit[[}\DecValTok{2}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-45-1.pdf}

您可以在图表中看到以下内容：

\begin{itemize}
\item
  当 \(\texttt{validation\_split}=0\)
  时：整个训练数据在算法过程中损失会逐渐减少，并且在某个时刻趋于平稳。训练的轮次越多，损失越小。
\item
  当 \(\texttt{validation\_split}>0\)
  时：训练和验证数据的损失会分别减少。训练数据的损失预计会减少并趋于平稳，而验证数据的损失会减少并在某个时刻再次增加。此时，损失最小的轮次是选择最佳训练轮次的良好依据。
\end{itemize}

下面，我们展示了两个示例：

as follows, closely copy paste what you find on the linked websites as
comments:

\begin{figure}
\centering
\includegraphics{./Figs/Case2/Case2-Fig3.png}
\caption{神经网络训练结果示例1}
\end{figure}

\begin{figure}
\centering
\includegraphics{./Figs/Case2/Case2-Fig4.png}
\caption{神经网络训练结果示例2}
\end{figure}

在第一个图表中，训练损失随着每个 epoch
而减少。这就是你在运行梯度下降优化时所期望的 ------
你试图最小化的数量应该随着每次迭代而减少。但验证损失和准确性并非如此：它们似乎在第四个
epoch
达到顶峰。这是我们之前警告过的一个例子：在训练数据上表现更好的模型不一定会在从未见过的数据上表现更好。准确地说，你看到的是过拟合：在第二个
epoch
之后，你对训练数据进行了过度优化，你最终学习到的是特定于训练数据的表示，而不是泛化到训练之外的数据。

在第二张图中，学习数据 \(\mathcal{D}\)
被分成训练集和验证集。训练集用于拟合模型，验证集用于（样本外）验证。我们强调，我们选择的验证集与测试集
是分开的。在第二张图中，学习数据以 8:2
的比例分为训练集（蓝色）和验证集（绿色）。该网络适合训练集，并在验证集上得到验证。我们观察到模型在大约
150 个 epochs 后开始过拟合，因为此后验证损失（绿色）开始增加。

\textbf{练习：}
在这里的代码中，我们看到了基于训练集和验证集的图表。拿这个模型，看看测试数据上的曲线。

\textbf{练习：} 为了比较模型，应该选择上面选择的最佳的 epochs
数量。这样做并查看样本外预测如何变化。

\subsubsection{验证模型}\label{ux9a8cux8bc1ux6a21ux578b-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculating the predictions}
\NormalTok{train}\SpecialCharTok{$}\NormalTok{fitshNN }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(model\_sh }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{predict}\NormalTok{(}\FunctionTok{list}\NormalTok{(Xtrain, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure)))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 16948/16948 - 9s - 9s/epoch - 524us/step
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test}\SpecialCharTok{$}\NormalTok{fitshNN }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(model\_sh }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{predict}\NormalTok{(}\FunctionTok{list}\NormalTok{(Xtest, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure)))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 4240/4240 - 3s - 3s/epoch - 626us/step
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# average in{-}sample and out{-}of{-}sample losses (in 10\^{}({-}2))}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"100 x Poisson deviance shallow network (train): \%s"}\NormalTok{, }\FunctionTok{PoissonDeviance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{fitshNN, train}\SpecialCharTok{$}\NormalTok{ClaimNb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "100 x Poisson deviance shallow network (train): 25.2044477402821"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"100 x Poisson deviance shallow network (test): \%s"}\NormalTok{, }\FunctionTok{PoissonDeviance}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitshNN, test}\SpecialCharTok{$}\NormalTok{ClaimNb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "100 x Poisson deviance shallow network (test): 25.3483497426932"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# average frequency}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Average frequency (test): \%s"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitshNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Average frequency (test): 0.0736"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainable\_params }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(model\_sh}\SpecialCharTok{$}\NormalTok{trainable\_weights, k\_count\_params)))}
\NormalTok{df\_cmp }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{model =} \StringTok{"M2: Shallow Plain Network"}\NormalTok{, }\AttributeTok{epochs =}\NormalTok{ epochs,}
             \AttributeTok{run\_time =} \FunctionTok{round}\NormalTok{(exec\_time[[}\DecValTok{3}\NormalTok{]], }\DecValTok{0}\NormalTok{), }\AttributeTok{parameters =}\NormalTok{ trainable\_params,}
             \AttributeTok{in\_sample\_loss =} \FunctionTok{round}\NormalTok{(}\FunctionTok{PoissonDeviance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{fitshNN, train}\SpecialCharTok{$}\NormalTok{ClaimNb), }\DecValTok{4}\NormalTok{),}
             \AttributeTok{out\_sample\_loss =} \FunctionTok{round}\NormalTok{(}\FunctionTok{PoissonDeviance}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitshNN, test}\SpecialCharTok{$}\NormalTok{ClaimNb), }\DecValTok{4}\NormalTok{),}
             \AttributeTok{avg\_freq =} \FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitshNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{)}
\NormalTok{  ))}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(df\_cmp)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2796}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0753}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0968}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1183}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1613}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1720}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0968}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
epochs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
run\_time
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
parameters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
in\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
out\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
avg\_freq
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M1: GLM & NA & 12 & 48 & 24.0875 & 24.1666 & 0.0737 \\
M2: Shallow Plain Network & 300 & 0 & 801 & 25.2044 & 25.3483 &
0.0736 \\
\end{longtable}

\subsubsection{校准模型}\label{ux6821ux51c6ux6a21ux578b-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Area}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"Area"}\NormalTok{, }\StringTok{"Frequency by area"}\NormalTok{, }\StringTok{"shNN"}\NormalTok{, }\StringTok{"fitshNN"}\NormalTok{)}
\CommentTok{\# VehPower}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehPower"}\NormalTok{, }\StringTok{"Frequency by vehicle power"}\NormalTok{, }\StringTok{"shNN"}\NormalTok{, }\StringTok{"fitshNN"}\NormalTok{)}
\CommentTok{\# VehBrand}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehBrand"}\NormalTok{, }\StringTok{"Frequency by vehicle brand"}\NormalTok{, }\StringTok{"shNN"}\NormalTok{, }\StringTok{"fitshNN"}\NormalTok{)}
\CommentTok{\# VehAge}
\NormalTok{p4 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehAge"}\NormalTok{, }\StringTok{"Frequency by vehicle age"}\NormalTok{, }\StringTok{"shNN"}\NormalTok{, }\StringTok{"fitshNN"}\NormalTok{)}

\FunctionTok{grid.arrange}\NormalTok{(p1, p2, p3, p4)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-48-1.pdf}

值得一提的是，拟合非常接近观察结果，并且神经网络消除了 VehAge
观察结果中的一些跳跃，这符合预期。

\textbf{练习：} 改变神经元个数，比较参数个数和拟合结果。

\textbf{练习：} 通过从输入中删除例如 Area、VehPower
来更改输入特征空间，并比较网络参数的数量和拟合结果。

\textbf{练习：} 改变激活函数（只在合适的地方）并比较结果。

\textbf{练习：}:
阅读优化器的文档并使用不同的优化器运行后续分析并比较结果。

\textbf{练习：} 更改随机种子（在教程开始时）并比较结果。

\textbf{练习：}
运行相同的分析，看看您是否可以完全重现结果。你从中得出什么结论？

\textbf{练习：} 更改 validation\_split 和 verbose
参数并查看模型拟合的差异。

\subsection{模型 3：深层普通神经网络
(dpNN)}\label{ux6a21ux578b-3ux6df1ux5c42ux666eux901aux795eux7ecfux7f51ux7edc-dpnn}

让我们拟合一个深度神经网络。在本章中，我们不会提供与上面一样多的说明，因为它们在本章中仍然有效。

在拟合和指定神经网络之前，我们强烈建议绘制它。这有助于使用
\texttt{keras} 函数。

我们选择了一个 \(K=3\) 个隐藏层和 \(q_1=20\), \(q_1=15\) and \(q_1=10\)
个神经元的网络，如下图所示：

\begin{figure}
\centering
\includegraphics{./Figs/Case2/Case2-Fig5.png}
\caption{深层神经网络结构图}
\end{figure}

输入层的维度由选定的输入特征维度定义（见上文）。

\subsubsection{定义}\label{ux5b9aux4e49-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define network}
\NormalTok{q0 }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(features)   }\CommentTok{\# dimension of features}
\NormalTok{q1 }\OtherTok{\textless{}{-}} \DecValTok{20}                 \CommentTok{\# number of neurons in first hidden layer}
\NormalTok{q2 }\OtherTok{\textless{}{-}} \DecValTok{15}                 \CommentTok{\# number of neurons in second hidden layer}
\NormalTok{q3 }\OtherTok{\textless{}{-}} \DecValTok{10}                 \CommentTok{\# number of neurons in second hidden layer}

\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Neural network with K=3 hidden layer"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Neural network with K=3 hidden layer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Input feature dimension: q0 = \%s"}\NormalTok{, q0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Input feature dimension: q0 = 38"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of hidden neurons first layer: q1 = \%s"}\NormalTok{, q1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of hidden neurons first layer: q1 = 20"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of hidden neurons second layer: q2 = \%s"}\NormalTok{, q2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of hidden neurons second layer: q2 = 15"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of hidden neurons third layer: q3 = \%s"}\NormalTok{, q3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of hidden neurons third layer: q3 = 10"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Output dimension: \%s"}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Output dimension: 1"
\end{verbatim}

下面我们定义具有 \(q_1\)
个隐藏神经元和双曲正切激活函数的前馈浅层网络，由于 Poisson
GLM，输出具有指数激活函数。

风险暴露数作为偏移量包含在内，我们使用同质模型来初始化输出。

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Design }\OtherTok{\textless{}{-}} \FunctionTok{layer\_input}\NormalTok{(}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(q0), }\AttributeTok{dtype =} \StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}Design\textquotesingle{}}\NormalTok{) }
\NormalTok{LogVol }\OtherTok{\textless{}{-}} \FunctionTok{layer\_input}\NormalTok{(}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{), }\AttributeTok{dtype =} \StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}LogVol\textquotesingle{}}\NormalTok{)}

\NormalTok{Network }\OtherTok{\textless{}{-}}\NormalTok{ Design }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =}\NormalTok{ q1, }\AttributeTok{activation =} \StringTok{\textquotesingle{}tanh\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}layer1\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =}\NormalTok{ q2, }\AttributeTok{activation =} \StringTok{\textquotesingle{}tanh\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}layer2\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =}\NormalTok{ q3, }\AttributeTok{activation =} \StringTok{\textquotesingle{}tanh\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}layer3\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}linear\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}Network\textquotesingle{}}\NormalTok{, }
              \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(}\FunctionTok{array}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(q3, }\DecValTok{1}\NormalTok{)), }\FunctionTok{array}\NormalTok{(}\FunctionTok{log}\NormalTok{(lambda\_hom), }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))))}

\NormalTok{Response }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(Network, LogVol) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_add}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =}\NormalTok{ k\_exp, }\AttributeTok{name =} \StringTok{\textquotesingle{}Response\textquotesingle{}}\NormalTok{, }\AttributeTok{trainable =} \ConstantTok{FALSE}\NormalTok{,}
              \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(}\FunctionTok{array}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)), }\FunctionTok{array}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))))}

\NormalTok{model\_dp }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model}\NormalTok{(}\AttributeTok{inputs =} \FunctionTok{c}\NormalTok{(Design, LogVol), }\AttributeTok{outputs =} \FunctionTok{c}\NormalTok{(Response))}
\end{Highlighting}
\end{Shaded}

\subsubsection{编译模型}\label{ux7f16ux8bd1ux6a21ux578b-1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_dp }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{loss =} \StringTok{\textquotesingle{}poisson\textquotesingle{}}\NormalTok{,}
  \AttributeTok{optimizer =}\NormalTok{ optimizers[}\DecValTok{7}\NormalTok{]}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(model\_dp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Model: "model_1"
## ________________________________________________________________________________
##  Layer (type)       Output Shape         Para   Connected to         Trainable  
##                                          m #                                    
## ================================================================================
##  Design (InputLaye  [(None, 38)]         0      []                   Y          
##  r)                                                                             
##  layer1 (Dense)     (None, 20)           780    ['Design[0][0]']     Y          
##  layer2 (Dense)     (None, 15)           315    ['layer1[0][0]']     Y          
##  layer3 (Dense)     (None, 10)           160    ['layer2[0][0]']     Y          
##  Network (Dense)    (None, 1)            11     ['layer3[0][0]']     Y          
##  LogVol (InputLaye  [(None, 1)]          0      []                   Y          
##  r)                                                                             
##  add_1 (Add)        (None, 1)            0      ['Network[0][0]',    Y          
##                                                  'LogVol[0][0]']                
##  Response (Dense)   (None, 1)            2      ['add_1[0][0]']      N          
## ================================================================================
## Total params: 1268 (4.95 KB)
## Trainable params: 1266 (4.95 KB)
## Non-trainable params: 2 (8.00 Byte)
## ________________________________________________________________________________
\end{verbatim}

\subsubsection{拟合模型}\label{ux62dfux5408ux6a21ux578b-2}

为了拟合 \texttt{keras} 模型及其参数，我们在此处参考 \texttt{fit}
的\href{https://keras.rstudio.com/reference/fit.html}{帮助文件}。在那里您可以找到有关
\texttt{validation\_split} 和 \texttt{verbose} 参数的详细信息。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select number of epochs and batch\_size}
\NormalTok{epochs }\OtherTok{\textless{}{-}} \DecValTok{300}
\NormalTok{batch\_size }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{validation\_split }\OtherTok{\textless{}{-}} \FloatTok{0.2}  \CommentTok{\# set to \textgreater{}0 to see train/validation loss in plot(fit)}
\NormalTok{verbose }\OtherTok{\textless{}{-}} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# expected run{-}time on Renku 8GB environment around 200 seconds}
\NormalTok{exec\_time }\OtherTok{\textless{}{-}} \FunctionTok{system.time}\NormalTok{(}
  \ControlFlowTok{if}\NormalTok{ (update\_code }\SpecialCharTok{==} \DecValTok{0}  \SpecialCharTok{\&} \FunctionTok{file.exists.local}\NormalTok{(}\StringTok{"fit\_model\_dp.Rdata"}\NormalTok{))\{}
    \FunctionTok{load.file.local}\NormalTok{(}\StringTok{"fit\_model\_dp.Rdata"}\NormalTok{)}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{    fit\_model\_dp }\OtherTok{\textless{}{-}}\NormalTok{  model\_dp }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{fit}\NormalTok{(}
      \FunctionTok{list}\NormalTok{(Xtrain, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure))), }\FunctionTok{as.matrix}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{ClaimNb),}
      \AttributeTok{epochs =}\NormalTok{ epochs,}
      \AttributeTok{batch\_size =}\NormalTok{ batch\_size,}
      \AttributeTok{validation\_split =}\NormalTok{ validation\_split,}
      \AttributeTok{verbose =}\NormalTok{ verbose}
\NormalTok{    )}
    \FunctionTok{save.file.local}\NormalTok{(fit\_model\_dp)}
\NormalTok{  \}}
\NormalTok{)}
\NormalTok{exec\_time[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  user.self   sys.self    elapsed user.child  sys.child 
##          0          0          0         NA         NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}}\NormalTok{ fit\_model\_dp}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-54-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_loss}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit[[}\DecValTok{2}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-55-1.pdf}

同样，大约 100 个 epochs 似乎是最佳的。

\subsubsection{验证模型}\label{ux9a8cux8bc1ux6a21ux578b-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Validation: Poisson deviance}
\NormalTok{train}\SpecialCharTok{$}\NormalTok{fitdpNN }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(model\_dp }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{predict}\NormalTok{(}\FunctionTok{list}\NormalTok{(Xtrain, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure)))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 16948/16948 - 9s - 9s/epoch - 533us/step
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test}\SpecialCharTok{$}\NormalTok{fitdpNN }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(model\_dp }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{predict}\NormalTok{(}\FunctionTok{list}\NormalTok{(Xtest, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure)))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 4240/4240 - 2s - 2s/epoch - 536us/step
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"100 x Poisson deviance shallow network (train): \%s"}\NormalTok{, }\FunctionTok{PoissonDeviance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{fitdpNN, train}\SpecialCharTok{$}\NormalTok{ClaimNb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "100 x Poisson deviance shallow network (train): 25.2044477402821"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"100 x Poisson deviance shallow network (test): \%s"}\NormalTok{, }\FunctionTok{PoissonDeviance}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitdpNN, test}\SpecialCharTok{$}\NormalTok{ClaimNb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "100 x Poisson deviance shallow network (test): 25.3483497426932"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# average frequency}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Average frequency (test): \%s"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitdpNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Average frequency (test): 0.0736"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainable\_params }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(model\_dp}\SpecialCharTok{$}\NormalTok{trainable\_weights, k\_count\_params)))}
\NormalTok{df\_cmp }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{model =} \StringTok{"M3: Deep Plain Network"}\NormalTok{, }\AttributeTok{epochs =}\NormalTok{ epochs,}
             \AttributeTok{run\_time =} \FunctionTok{round}\NormalTok{(exec\_time[[}\DecValTok{3}\NormalTok{]], }\DecValTok{0}\NormalTok{), }\AttributeTok{parameters =}\NormalTok{ trainable\_params,}
             \AttributeTok{in\_sample\_loss =} \FunctionTok{round}\NormalTok{(}\FunctionTok{PoissonDeviance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{fitdpNN, train}\SpecialCharTok{$}\NormalTok{ClaimNb), }\DecValTok{4}\NormalTok{),}
             \AttributeTok{out\_sample\_loss =} \FunctionTok{round}\NormalTok{(}\FunctionTok{PoissonDeviance}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitdpNN, test}\SpecialCharTok{$}\NormalTok{ClaimNb), }\DecValTok{4}\NormalTok{),}
             \AttributeTok{avg\_freq =} \FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitdpNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{)}
\NormalTok{  ))}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(df\_cmp)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2796}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0753}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0968}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1183}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1613}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1720}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0968}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
epochs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
run\_time
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
parameters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
in\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
out\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
avg\_freq
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M1: GLM & NA & 12 & 48 & 24.0875 & 24.1666 & 0.0737 \\
M2: Shallow Plain Network & 300 & 0 & 801 & 25.2044 & 25.3483 &
0.0736 \\
M3: Deep Plain Network & 300 & 0 & 1266 & 25.2044 & 25.3483 & 0.0736 \\
\end{longtable}

\subsubsection{校准模型}\label{ux6821ux51c6ux6a21ux578b-2}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Area}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"Area"}\NormalTok{, }\StringTok{"frequency by area"}\NormalTok{, }\StringTok{"dpNN"}\NormalTok{, }\StringTok{"fitdpNN"}\NormalTok{)}
\CommentTok{\# VehPower}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehPower"}\NormalTok{, }\StringTok{"frequency by vehicle power"}\NormalTok{, }\StringTok{"dpNN"}\NormalTok{, }\StringTok{"fitdpNN"}\NormalTok{)}
\CommentTok{\# VehBrand}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehBrand"}\NormalTok{, }\StringTok{"frequency by vehicle brand"}\NormalTok{, }\StringTok{"dpNN"}\NormalTok{, }\StringTok{"fitdpNN"}\NormalTok{)}
\CommentTok{\# VehAge}
\NormalTok{p4 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehAge"}\NormalTok{, }\StringTok{"frequency by vehicle age"}\NormalTok{, }\StringTok{"dpNN"}\NormalTok{, }\StringTok{"fitdpNN"}\NormalTok{)}

\FunctionTok{grid.arrange}\NormalTok{(p1, p2, p3, p4)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-58-1.pdf}

\subsection{模型 4：包含 dropout 层的深度神经网络
(drNN)}\label{ux6a21ux578b-4ux5305ux542b-dropout-ux5c42ux7684ux6df1ux5ea6ux795eux7ecfux7f51ux7edc-drnn}

\subsubsection{定义}\label{ux5b9aux4e49-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define network}
\NormalTok{q0 }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(features)  }\CommentTok{\# dimension of input features}
\NormalTok{q1 }\OtherTok{\textless{}{-}} \DecValTok{20}                \CommentTok{\# number of neurons in first hidden layer}
\NormalTok{q2 }\OtherTok{\textless{}{-}} \DecValTok{15}                \CommentTok{\# number of neurons in second hidden layer}
\NormalTok{q3 }\OtherTok{\textless{}{-}} \DecValTok{10}                \CommentTok{\# number of neurons in second hidden layer}
\NormalTok{p0 }\OtherTok{\textless{}{-}} \FloatTok{0.05}              \CommentTok{\# dropout rate     }

\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Neural network with K=3 hidden layer and 3 dropout layers"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Neural network with K=3 hidden layer and 3 dropout layers"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Input feature dimension: q0 = \%s"}\NormalTok{, q0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Input feature dimension: q0 = 38"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of hidden neurons first layer: q1 = \%s"}\NormalTok{, q1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of hidden neurons first layer: q1 = 20"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of hidden neurons second layer: q2 = \%s"}\NormalTok{, q2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of hidden neurons second layer: q2 = 15"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Number of hidden neurons third layer: q3 = \%s"}\NormalTok{, q3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Number of hidden neurons third layer: q3 = 10"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Output dimension: \%s"}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Output dimension: 1"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Design }\OtherTok{\textless{}{-}} \FunctionTok{layer\_input}\NormalTok{(}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(q0), }\AttributeTok{dtype =} \StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}Design\textquotesingle{}}\NormalTok{)}
\NormalTok{LogVol }\OtherTok{\textless{}{-}} \FunctionTok{layer\_input}\NormalTok{(}\AttributeTok{shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{), }\AttributeTok{dtype =} \StringTok{\textquotesingle{}float32\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}LogVol\textquotesingle{}}\NormalTok{)}

\NormalTok{Network }\OtherTok{\textless{}{-}}\NormalTok{ Design }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =}\NormalTok{ q1, }\AttributeTok{activation =} \StringTok{\textquotesingle{}tanh\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}layer1\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =}\NormalTok{ p0) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =}\NormalTok{ q2, }\AttributeTok{activation =} \StringTok{\textquotesingle{}tanh\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}layer2\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =}\NormalTok{ p0) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =}\NormalTok{ q3, }\AttributeTok{activation =} \StringTok{\textquotesingle{}tanh\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}layer3\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dropout}\NormalTok{(}\AttributeTok{rate =}\NormalTok{ p0) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{\textquotesingle{}linear\textquotesingle{}}\NormalTok{, }\AttributeTok{name =} \StringTok{\textquotesingle{}Network\textquotesingle{}}\NormalTok{, }
              \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(}\FunctionTok{array}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(q3, }\DecValTok{1}\NormalTok{)), }\FunctionTok{array}\NormalTok{(}\FunctionTok{log}\NormalTok{(lambda\_hom), }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))))}

\NormalTok{Response }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(Network, LogVol) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_add}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =}\NormalTok{ k\_exp, }\AttributeTok{name =} \StringTok{\textquotesingle{}Response\textquotesingle{}}\NormalTok{, }\AttributeTok{trainable =} \ConstantTok{FALSE}\NormalTok{, }
              \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(}\FunctionTok{array}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)), }\FunctionTok{array}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{))))}

\NormalTok{model\_dr }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model}\NormalTok{(}\AttributeTok{inputs =} \FunctionTok{c}\NormalTok{(Design, LogVol), }\AttributeTok{outputs =} \FunctionTok{c}\NormalTok{(Response))}
\end{Highlighting}
\end{Shaded}

\subsubsection{编译模型}\label{ux7f16ux8bd1ux6a21ux578b-2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_dr }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{loss =} \StringTok{\textquotesingle{}poisson\textquotesingle{}}\NormalTok{,}
  \AttributeTok{optimizer =}\NormalTok{ optimizers[}\DecValTok{7}\NormalTok{]}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(model\_dr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Model: "model_2"
## ________________________________________________________________________________
##  Layer (type)       Output Shape         Para   Connected to         Trainable  
##                                          m #                                    
## ================================================================================
##  Design (InputLaye  [(None, 38)]         0      []                   Y          
##  r)                                                                             
##  layer1 (Dense)     (None, 20)           780    ['Design[0][0]']     Y          
##  dropout_2 (Dropou  (None, 20)           0      ['layer1[0][0]']     Y          
##  t)                                                                             
##  layer2 (Dense)     (None, 15)           315    ['dropout_2[0][0]'   Y          
##                                                 ]                               
##  dropout_1 (Dropou  (None, 15)           0      ['layer2[0][0]']     Y          
##  t)                                                                             
##  layer3 (Dense)     (None, 10)           160    ['dropout_1[0][0]'   Y          
##                                                 ]                               
##  dropout (Dropout)  (None, 10)           0      ['layer3[0][0]']     Y          
##  Network (Dense)    (None, 1)            11     ['dropout[0][0]']    Y          
##  LogVol (InputLaye  [(None, 1)]          0      []                   Y          
##  r)                                                                             
##  add_2 (Add)        (None, 1)            0      ['Network[0][0]',    Y          
##                                                  'LogVol[0][0]']                
##  Response (Dense)   (None, 1)            2      ['add_2[0][0]']      N          
## ================================================================================
## Total params: 1268 (4.95 KB)
## Trainable params: 1266 (4.95 KB)
## Non-trainable params: 2 (8.00 Byte)
## ________________________________________________________________________________
\end{verbatim}

\subsubsection{拟合模型}\label{ux62dfux5408ux6a21ux578b-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select number of epochs and batch\_size}
\NormalTok{epochs }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{batch\_size }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{validation\_split }\OtherTok{\textless{}{-}} \FloatTok{0.2}  \CommentTok{\# set \textgreater{}0 to see train/validation loss in plot(fit)}
\NormalTok{verbose }\OtherTok{\textless{}{-}} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# expected run{-}time on Renku 8GB environment around 110 seconds (for 100 epochs)}
\NormalTok{exec\_time }\OtherTok{\textless{}{-}} \FunctionTok{system.time}\NormalTok{(}
  \ControlFlowTok{if}\NormalTok{ (update\_code }\SpecialCharTok{==} \DecValTok{0}  \SpecialCharTok{\&} \FunctionTok{file.exists.local}\NormalTok{(}\StringTok{"fit\_model\_dr.Rdata"}\NormalTok{))\{}
    \FunctionTok{load.file.local}\NormalTok{(}\StringTok{"fit\_model\_dr.Rdata"}\NormalTok{)}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{    fit\_model\_dr }\OtherTok{\textless{}{-}}\NormalTok{  model\_dr }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{fit}\NormalTok{(}
      \FunctionTok{list}\NormalTok{(Xtrain, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure))), }\FunctionTok{as.matrix}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{ClaimNb),}
      \AttributeTok{epochs =}\NormalTok{ epochs,}
      \AttributeTok{batch\_size =}\NormalTok{ batch\_size,}
      \AttributeTok{validation\_split =}\NormalTok{ validation\_split,}
      \AttributeTok{verbose =}\NormalTok{ verbose}
\NormalTok{    )}
    \FunctionTok{save.file.local}\NormalTok{(fit\_model\_dr)}
\NormalTok{  \}}
\NormalTok{)}
\NormalTok{exec\_time[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  user.self   sys.self    elapsed user.child  sys.child 
##          0          0          0         NA         NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}}\NormalTok{ fit\_model\_dr}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-64-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_loss}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fit[[}\DecValTok{2}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-65-1.pdf}

\subsubsection{验证模型}\label{ux9a8cux8bc1ux6a21ux578b-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Validation: Poisson deviance}
\NormalTok{train}\SpecialCharTok{$}\NormalTok{fitdrNN }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(model\_dr }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{predict}\NormalTok{(}\FunctionTok{list}\NormalTok{(Xtrain, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{Exposure)))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 16948/16948 - 9s - 9s/epoch - 539us/step
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test}\SpecialCharTok{$}\NormalTok{fitdrNN }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(model\_dr }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{predict}\NormalTok{(}\FunctionTok{list}\NormalTok{(Xtest, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{log}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure)))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 4240/4240 - 2s - 2s/epoch - 546us/step
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"100 x Poisson deviance shallow network (train): \%s"}\NormalTok{, }\FunctionTok{PoissonDeviance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{fitdrNN, train}\SpecialCharTok{$}\NormalTok{ClaimNb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "100 x Poisson deviance shallow network (train): 25.2044477402821"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"100 x Poisson deviance shallow network (test): \%s"}\NormalTok{, }\FunctionTok{PoissonDeviance}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitdrNN, test}\SpecialCharTok{$}\NormalTok{ClaimNb))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "100 x Poisson deviance shallow network (test): 25.3483497426932"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# average frequency}
\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Average frequency (test): \%s"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitdrNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Average frequency (test): 0.0736"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainable\_params }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(model\_dr}\SpecialCharTok{$}\NormalTok{trainable\_weights, k\_count\_params)))}
\NormalTok{df\_cmp }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{model =} \StringTok{"M4: Deep Dropout Network"}\NormalTok{, }\AttributeTok{epochs =}\NormalTok{ epochs,}
             \AttributeTok{run\_time =} \FunctionTok{round}\NormalTok{(exec\_time[[}\DecValTok{3}\NormalTok{]], }\DecValTok{0}\NormalTok{), }\AttributeTok{parameters =}\NormalTok{ trainable\_params,}
             \AttributeTok{in\_sample\_loss =} \FunctionTok{round}\NormalTok{(}\FunctionTok{PoissonDeviance}\NormalTok{(train}\SpecialCharTok{$}\NormalTok{fitdrNN, train}\SpecialCharTok{$}\NormalTok{ClaimNb), }\DecValTok{4}\NormalTok{),}
             \AttributeTok{out\_sample\_loss =} \FunctionTok{round}\NormalTok{(}\FunctionTok{PoissonDeviance}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitdrNN, test}\SpecialCharTok{$}\NormalTok{ClaimNb), }\DecValTok{4}\NormalTok{),}
             \AttributeTok{avg\_freq =} \FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{fitdrNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(test}\SpecialCharTok{$}\NormalTok{Exposure), }\DecValTok{4}\NormalTok{)}
\NormalTok{  ))}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(df\_cmp)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2796}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0753}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0968}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1183}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1613}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1720}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0968}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
epochs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
run\_time
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
parameters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
in\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
out\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
avg\_freq
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M1: GLM & NA & 12 & 48 & 24.0875 & 24.1666 & 0.0737 \\
M2: Shallow Plain Network & 300 & 0 & 801 & 25.2044 & 25.3483 &
0.0736 \\
M3: Deep Plain Network & 300 & 0 & 1266 & 25.2044 & 25.3483 & 0.0736 \\
M4: Deep Dropout Network & 500 & 0 & 1266 & 25.2044 & 25.3483 &
0.0736 \\
\end{longtable}

\subsubsection{校准模型}\label{ux6821ux51c6ux6a21ux578b-3}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Area}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"Area"}\NormalTok{, }\StringTok{"frequency by area"}\NormalTok{, }\StringTok{"drNN"}\NormalTok{, }\StringTok{"fitdrNN"}\NormalTok{)}
\CommentTok{\# VehPower}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehPower"}\NormalTok{, }\StringTok{"frequency by vehicle power"}\NormalTok{, }\StringTok{"drNN"}\NormalTok{, }\StringTok{"fitdrNN"}\NormalTok{)}
\CommentTok{\# VehBrand}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehBrand"}\NormalTok{, }\StringTok{"frequency by vehicle brand"}\NormalTok{, }\StringTok{"drNN"}\NormalTok{, }\StringTok{"fitdrNN"}\NormalTok{)}
\CommentTok{\# VehAge}
\NormalTok{p4 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_freq}\NormalTok{(test, }\StringTok{"VehAge"}\NormalTok{, }\StringTok{"frequency by vehicle age"}\NormalTok{, }\StringTok{"drNN"}\NormalTok{, }\StringTok{"fitdrNN"}\NormalTok{)}

\FunctionTok{grid.arrange}\NormalTok{(p1, p2, p3, p4)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-68-1.pdf}

\subsection{更多网络架构}\label{ux66f4ux591aux7f51ux7edcux67b6ux6784}

到目前为止，我们已经拟合了三种不同的网络架构，并且我们正在比较它们的性能和拟合质量。

如上所述，可以修改和更改超参数（优化器、批量大小、epochs等）并查看结果如何变化。

还可以更改神经网络架构，例如：改变层数和神经元数，添加额外的层（例如，标准化层），添加岭正则化等。

请参阅：

\begin{itemize}
\tightlist
\item
  \texttt{keras}
  速查表：\url{https://github.com/rstudio/cheatsheets/raw/master/keras.pdf}
\end{itemize}

\textbf{Exercise:} 查看 \texttt{keras}
速查表，研究标准化层并添加它们并比较结果。

\textbf{Exercise:} 查看其他层，尝试理解它们并比较这些层的影响。

\textbf{Exercise:} 应用岭正则化，了解它是什么，并比较结果。

\textbf{Exercise:} 在上述所有模型中，按照描述选择最佳 epochs
数并比较总体结果。

对于拟合神经网络，会出现几个问题，我们想在这里分享一些我们的经验：

\begin{itemize}
\item
  网络架构的选择可以被认为是艺术而不是科学。
\item
  神经网络将特征工程的挑战（网络自行学习）转移到选择架构的挑战上。
\item
  有一些关于架构的经验法则：

  \begin{itemize}
  \tightlist
  \item
    对于结构化数据，只需要3-5层，更多的层不会进一步提高准确性。
  \item
    对于发现交互作用，第三层及更高层会``考虑''它们。前两层则是针对``主要效应''。
  \end{itemize}
\item
  对缺失值的处理仍是一个未解决的问题。
\end{itemize}

\subsection{模型比较}\label{ux6a21ux578bux6bd4ux8f83}

各种模型的结果如下：

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(df\_cmp)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2796}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0753}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0968}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1183}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1613}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1720}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0968}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
epochs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
run\_time
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
parameters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
in\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
out\_sample\_loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
avg\_freq
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M1: GLM & NA & 12 & 48 & 24.0875 & 24.1666 & 0.0737 \\
M2: Shallow Plain Network & 300 & 0 & 801 & 25.2044 & 25.3483 &
0.0736 \\
M3: Deep Plain Network & 300 & 0 & 1266 & 25.2044 & 25.3483 & 0.0736 \\
M4: Deep Dropout Network & 500 & 0 & 1266 & 25.2044 & 25.3483 &
0.0736 \\
\end{longtable}

我们可以得出以下结论：

\begin{itemize}
\tightlist
\item
  模型M3和M4的样本外损失明显优于M1和M2。
\item
  M3和M4的更好表现表明M1和M2中缺少了一些交互作用。这与一般的理解相一致，即在有3个隐藏层时可以捕获交互作用。
\item
  浅层简单网络似乎不能适当地建模交互作用，而它们也不包括在GLM中。
\item
  神经网络模型之间的拟合平均索赔频率有所不同。这与GLM的情况不同，在GLM中，所有模型的预测索赔频率是相同的。这是神经网络的所谓\textbf{偏差正则化}问题。
\item
  我们看到 \texttt{freqMTPL2freq}
  数据集可能并不理想，也不代表标准的初级保险定价数据集，因为：

  \begin{itemize}
  \tightlist
  \item
    各种模型的性能非常相似，没有许多区域存在高度不同的预测。
  \item
    各种分类特征级别之间的边际预测频率变化不大。
  \item
    教程的目的是演示技术并进行比较，而不是很好地识别重要特征。
  \end{itemize}
\end{itemize}

\subsubsection{校准}\label{ux6821ux51c6}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_cmp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(xvar, title, }\AttributeTok{maxlim =} \FloatTok{0.35}\NormalTok{) \{}
\NormalTok{  out }\OtherTok{\textless{}{-}}\NormalTok{ test }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(}\SpecialCharTok{!!}\FunctionTok{sym}\NormalTok{(xvar)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{vol =} \FunctionTok{sum}\NormalTok{(Exposure),}
    \AttributeTok{obs =} \FunctionTok{sum}\NormalTok{(ClaimNb) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Exposure),}
    \AttributeTok{glm =} \FunctionTok{sum}\NormalTok{(fitGLM2) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Exposure),}
    \AttributeTok{shNN =} \FunctionTok{sum}\NormalTok{(fitshNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Exposure),}
    \AttributeTok{dpNN =} \FunctionTok{sum}\NormalTok{(fitdpNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Exposure),}
    \AttributeTok{drNN =} \FunctionTok{sum}\NormalTok{(fitdrNN) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Exposure)}
\NormalTok{  )}
  
\NormalTok{  max\_pri }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(out}\SpecialCharTok{$}\NormalTok{obs, out}\SpecialCharTok{$}\NormalTok{glm, out}\SpecialCharTok{$}\NormalTok{shNN, out}\SpecialCharTok{$}\NormalTok{dpNN, out}\SpecialCharTok{$}\NormalTok{drNN)}
\NormalTok{  max\_sec }\OtherTok{\textless{}{-}} \FloatTok{1.1} \SpecialCharTok{*} \FunctionTok{max}\NormalTok{(out}\SpecialCharTok{$}\NormalTok{vol)}
\NormalTok{  max\_ratio }\OtherTok{\textless{}{-}}\NormalTok{ max\_pri }\SpecialCharTok{/}\NormalTok{ max\_sec}
  
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(maxlim)) maxlim }\OtherTok{\textless{}{-}}\NormalTok{ max\_pri}
  
  \FunctionTok{ggplot}\NormalTok{(out, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \SpecialCharTok{!!}\FunctionTok{sym}\NormalTok{(xvar), }\AttributeTok{group =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ obs, }\AttributeTok{colour =} \StringTok{"observed"}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ obs, }\AttributeTok{colour =} \StringTok{"observed"}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ glm, }\AttributeTok{colour =} \StringTok{"GLM"}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ glm, }\AttributeTok{colour =} \StringTok{"GLM"}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ shNN, }\AttributeTok{colour =} \StringTok{"shNN"}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ shNN, }\AttributeTok{colour =} \StringTok{"shNN"}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ dpNN, }\AttributeTok{colour =} \StringTok{"dpNN"}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ dpNN, }\AttributeTok{colour =} \StringTok{"dpNN"}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ drNN, }\AttributeTok{colour =} \StringTok{"drNN"}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ drNN, }\AttributeTok{colour =} \StringTok{"drNN"}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ vol }\SpecialCharTok{*}\NormalTok{ (max\_ratio)), }\AttributeTok{colour =} \StringTok{"grey"}\NormalTok{, }\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Frequency"}\NormalTok{, }\AttributeTok{sec.axis =} \FunctionTok{sec\_axis}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{/}\NormalTok{ (max\_ratio), }\AttributeTok{name =} \StringTok{"Exposure"}\NormalTok{), }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, maxlim)) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =}\NormalTok{ xvar, }\AttributeTok{title =}\NormalTok{ title) }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{, }\AttributeTok{legend.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{6}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Area}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_cmp}\NormalTok{(}\StringTok{"Area"}\NormalTok{, }\StringTok{"Frequency and Exposure by Area"}\NormalTok{)}
\CommentTok{\# VehPower}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_cmp}\NormalTok{(}\StringTok{"VehPower"}\NormalTok{, }\StringTok{"Frequency and Exposure by VehPower"}\NormalTok{)}
\CommentTok{\# VehBrand}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_cmp}\NormalTok{(}\StringTok{"VehBrand"}\NormalTok{, }\StringTok{"Frequency and Exposure by VehBrand"}\NormalTok{)}
\CommentTok{\# VehAge}
\NormalTok{p4 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_cmp}\NormalTok{(}\StringTok{"VehAge"}\NormalTok{, }\StringTok{"Frequency and Exposure by VehAge"}\NormalTok{)}
\CommentTok{\# DrivAge plot with exposure distribution}
\NormalTok{p5 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_cmp}\NormalTok{(}\StringTok{"DrivAge"}\NormalTok{, }\StringTok{"Frequency and Exposure by DrivAge"}\NormalTok{)}

\FunctionTok{grid.arrange}\NormalTok{(p1, p2, p3, p4, p5)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-70-1.pdf}

样本外索赔频率预测（在对数尺度上）比较：

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_claims\_freq }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(xvar, yvar, xlab, ylab) \{}
\NormalTok{  axis\_min }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{max}\NormalTok{(test[[xvar]], test[[yvar]]))}
\NormalTok{  axis\_max }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{min}\NormalTok{(test[[xvar]], test[[yvar]]))}
  
  \FunctionTok{ggplot}\NormalTok{(test, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{log}\NormalTok{(}\SpecialCharTok{!!}\FunctionTok{sym}\NormalTok{(xvar)), }\AttributeTok{y =} \FunctionTok{log}\NormalTok{(}\SpecialCharTok{!!}\FunctionTok{sym}\NormalTok{(yvar)), }\AttributeTok{colour =}\NormalTok{ Exposure)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{colour =} \StringTok{"\#000000"}\NormalTok{, }\AttributeTok{slope =} \DecValTok{1}\NormalTok{, }\AttributeTok{intercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{xlim}\NormalTok{(axis\_max, axis\_min) }\SpecialCharTok{+} \FunctionTok{ylim}\NormalTok{(axis\_max, axis\_min) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =}\NormalTok{ xlab, }\AttributeTok{y =}\NormalTok{ ylab, }\AttributeTok{title =} \StringTok{"Claims frequency prediction (log{-}scale)"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_colour\_gradient}\NormalTok{(}\AttributeTok{low =} \StringTok{"green"}\NormalTok{, }\AttributeTok{high =} \StringTok{"red"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_claims\_freq}\NormalTok{(}\StringTok{"fitshNN"}\NormalTok{, }\StringTok{"fitdpNN"}\NormalTok{, }\StringTok{"shNN"}\NormalTok{, }\StringTok{"dpNN"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-72-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_claims\_freq}\NormalTok{(}\StringTok{"fitshNN"}\NormalTok{, }\StringTok{"fitGLM2"}\NormalTok{, }\StringTok{"shNN"}\NormalTok{, }\StringTok{"GLM"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-73-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_claims\_freq}\NormalTok{(}\StringTok{"fitdpNN"}\NormalTok{, }\StringTok{"fitGLM2"}\NormalTok{, }\StringTok{"dpNN"}\NormalTok{, }\StringTok{"GLM"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-74-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_claims\_freq}\NormalTok{(}\StringTok{"fitdrNN"}\NormalTok{, }\StringTok{"fitdpNN"}\NormalTok{, }\StringTok{"drNN"}\NormalTok{, }\StringTok{"dpNN"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{freMTPLfreq_fnn-zh-cn_files/figure-latex/unnamed-chunk-75-1.pdf}
\textbf{练习：} \texttt{keras(...)} 函数也可以用来拟合广义线性模型
(GLM)。采用浅层网络结构，减少隐藏层并改为拟合GLM。然后将其与使用
\texttt{glm()} 函数的相应输出进行比较。

\textbf{练习：} 作为拟合的初始权重，我们使用同质模型拟合（存储在
变量\texttt{glmHom} 中）。检查输出对该输入参数变化的依赖性。

\section{版本信息}\label{ux7248ux672cux4fe1ux606f}

该文件是使用以下软件包生成的。

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sessionInfo}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## R version 4.3.1 (2023-06-16 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 11 x64 (build 22631)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=Chinese (Simplified)_China.utf8 
## [2] LC_CTYPE=Chinese (Simplified)_China.utf8   
## [3] LC_MONETARY=Chinese (Simplified)_China.utf8
## [4] LC_NUMERIC=C                               
## [5] LC_TIME=Chinese (Simplified)_China.utf8    
## 
## time zone: Asia/Shanghai
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] repr_1.1.7       tidyr_1.3.1      splitTools_1.0.1 gridExtra_2.3   
##  [5] ggplot2_3.5.1    purrr_1.0.2      tibble_3.2.1     dplyr_1.1.4     
##  [9] magrittr_2.0.3   keras_2.13.0    
## 
## loaded via a namespace (and not attached):
##  [1] rappdirs_0.3.3    utf8_1.2.3        generics_0.1.3    lattice_0.21-8   
##  [5] digest_0.6.33     evaluate_0.21     grid_4.3.1        fastmap_1.1.1    
##  [9] rprojroot_2.0.3   jsonlite_1.8.7    Matrix_1.6-1      whisker_0.4.1    
## [13] tfruns_1.5.3      mgcv_1.8-42       tensorflow_2.16.0 fansi_1.0.4      
## [17] scales_1.3.0      cli_3.6.1         rlang_1.1.1       splines_4.3.1    
## [21] munsell_0.5.0     base64enc_0.1-3   withr_2.5.0       yaml_2.3.7       
## [25] tools_4.3.1       colorspace_2.1-0  zeallot_0.1.0     here_1.0.1       
## [29] reticulate_1.38.0 vctrs_0.6.5       R6_2.5.1          png_0.1-8        
## [33] lifecycle_1.0.3   pkgconfig_2.0.3   pillar_1.9.0      gtable_0.3.4     
## [37] glue_1.6.2        Rcpp_1.0.11       xfun_0.40         tidyselect_1.2.0 
## [41] highr_0.10        rstudioapi_0.16.0 knitr_1.43        farver_2.1.1     
## [45] nlme_3.1-162      htmltools_0.5.6   rmarkdown_2.24    labeling_0.4.3   
## [49] compiler_4.3.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reticulate}\SpecialCharTok{::}\FunctionTok{py\_config}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## python:         G:/Anaconda3/envs/tfenv/python.exe
## libpython:      G:/Anaconda3/envs/tfenv/python311.dll
## pythonhome:     G:/Anaconda3/envs/tfenv
## version:        3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:47:18) [MSC v.1916 64 bit (AMD64)]
## Architecture:   64bit
## numpy:          G:/Anaconda3/envs/tfenv/Lib/site-packages/numpy
## numpy_version:  1.24.3
## tensorflow:     G:\Anaconda3\envs\tfenv\Lib\site-packages\tensorflow\__init__.p
## 
## NOTE: Python version was forced by RETICULATE_PYTHON_FALLBACK
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tensorflow}\SpecialCharTok{::}\FunctionTok{tf\_version}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] '2.13'
\end{verbatim}

\subsection{参考资料}\label{ux53c2ux8003ux8d44ux6599}

\begin{itemize}
\tightlist
\item
  \url{https://tensorflow.rstudio.com/guide/}
\item
  \url{https://github.com/rstudio/cheatsheets/raw/master/keras.pdf}
\item
  \url{https://cran.r-project.org/web/packages/keras/vignettes/guide_keras.html}
\item
  \url{https://keras.rstudio.com/articles/about_keras_models.html}
\item
  \url{https://keras.rstudio.com/articles/functional_api.html}
\item
  \url{https://cran.rstudio.com/web/packages/keras/vignettes/sequential_model.html}
\item
  \url{https://www.rdocumentation.org/packages/keras/versions/2.3.0.0/topics/layer_dense}
\item
  \url{https://www.rdocumentation.org/packages/keras/versions/2.1.6/topics/compile}
\end{itemize}

\end{document}
